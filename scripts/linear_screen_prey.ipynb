{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prey notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import neat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import multimodal_mazes\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas \n",
    "\n",
    "* Odour - constant but noisy. \n",
    "* Sound - reliable but infrequent.\n",
    "* Rather than resetting the env to zero every step, you could decay it. E.g. env[:,:,:-1] *= 0.8 + blur. \n",
    "* Analysis: n_prey caught, speed, costs (e.g. movement vs food). \n",
    "* Evolve prey against different algorithms. Then, evolve predators against these prey.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "width=21\n",
    "height=21\n",
    "n_prey = 1\n",
    "n_steps = 100\n",
    "n_trials = 100\n",
    "pk = 30 # the width of the prey's Gaussian signal (in rc)\n",
    "visible_steps = n_steps\n",
    "scenario =  \"Constant\"\n",
    "motion = \"Linear\"\n",
    "multisensory = \"Unisensory\"\n",
    "\n",
    "if scenario == \"Static\":\n",
    "    pc = 0.0\n",
    "    pm = 0\n",
    "    pe = 1\n",
    "    motion = None\n",
    "elif scenario == \"Constant\":\n",
    "    pc = 0.0\n",
    "    pm = 1\n",
    "    pe = 0.998\n",
    "    noise = 0.002\n",
    "    case = \"2\"\n",
    "elif scenario == \"Random\":\n",
    "    pc = 0.0\n",
    "    pm = 1\n",
    "    pe = 0.2\n",
    "    motion = \"Levy\"\n",
    "    case = \"1\"\n",
    "elif scenario == \"Two Prey\":\n",
    "    pc = 0.0\n",
    "    pm = 0\n",
    "    pe = 0.998\n",
    "    noise = 0.002\n",
    "    multisensory = \"Balanced\"\n",
    "    case = \"4\"\n",
    "    n_prey = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness vs noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs noise\n",
    "noises = np.linspace(start=0.0, stop=2.0, num=10)\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "results = np.zeros((len(noises), len(policies)))\n",
    "\n",
    "# Test agents\n",
    "for a, noise in enumerate(tqdm(noises)):\n",
    "\n",
    "    for b, policy in enumerate(policies): \n",
    "        if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "        elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "            agnt.alpha = 0.6\n",
    "        elif policy == \"Levy\":\n",
    "            agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "        fitness, _, _, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=\"2 L\", motion=motion, pc=pc, pm=pm, pe=pe)\n",
    "\n",
    "        results[a, b] = fitness\n",
    "\n",
    "for b, policy in enumerate(policies): \n",
    "    plt.plot(noises, results[:,b], color=colors[b], label=policy)\n",
    "\n",
    "plt.ylim([0, 1.05])\n",
    "plt.ylabel('Fitness')\n",
    "plt.xlabel('Sensor Noise')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs noise AUC \n",
    "auc = np.trapz(y=results.T, x=noises, axis=1)\n",
    "for b, _ in enumerate(policies): \n",
    "    ml, sl, _ = plt.stem(b, auc[b] - auc[0])\n",
    "    ml.set_color(colors[b])\n",
    "    sl.set_color(colors[b])\n",
    "plt.xticks(range(len(policies)), policies, rotation='vertical')\n",
    "plt.ylabel('AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prey initial position vs capture time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prey initial position vs capture time\n",
    "noise = 0\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for b, policy in enumerate(policies): \n",
    "    if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "        agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "    elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "        agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "        agnt.alpha = 0.6\n",
    "    elif policy == \"Levy\":\n",
    "        agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "    _, times, _, preys = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=\"\", motion=motion, pc=pc, pm=pm, pe=pe) \n",
    "\n",
    "    percentage_results = []\n",
    "    capture_results = {str(key):[0,0] for key in range(pk//2, width+pk//2)}\n",
    "    \n",
    "    for prey in preys:\n",
    "        if prey[0].state == 0:\n",
    "            capture_results[str(prey[0].path[0][1])][1] += 1\n",
    "        capture_results[str(prey[0].path[0][1])][0] += 1\n",
    "    \n",
    "    for key,value in capture_results.items():\n",
    "        percentage = 0\n",
    "        if value[0] != 0:\n",
    "            percentage = (value[1]/value[0])*100\n",
    "        percentage_results.append([int(key), percentage])\n",
    "    \n",
    "    percentage_results = sorted(percentage_results)\n",
    "    percentage_positions = [percentage_results[it][0] for it in range(len(percentage_results))]\n",
    "    percentages = [percentage_results[it][1] for it in range(len(percentage_results))]\n",
    "\n",
    "    time_results = sorted([[preys[result][0].path[0][1], times[result]] for result in range(len(preys))]) # uncaptured prey have time set to maximum\n",
    "    time_positions = [time_results[it][0] for it in range(len(time_results))]\n",
    "    sorted_times = [time_results[it][1] for it in range(len(time_results))]\n",
    "    \n",
    "    curve1 = np.poly1d(np.polyfit(time_positions, sorted_times, deg=2))\n",
    "    y1 = curve1(time_positions) \n",
    "    y1[y1 <= 2] = 2\n",
    "    ax1.plot(time_positions, y1, color=colors[b], label=policies[b])\n",
    "\n",
    "    curve2 = np.poly1d(np.polyfit(percentage_positions, percentages, deg=2))\n",
    "    y2 = curve2(percentage_positions) \n",
    "    y2[y2 > 100] = 100\n",
    "    y2[y2 < 0] = 0\n",
    "    ax2.plot(percentage_positions, y2, color=colors[b])\n",
    "\n",
    "fig.suptitle(\"Preys Inital Position Correlation Capture Time and Percentage Caught\")\n",
    "fig.legend(loc='center right', bbox_to_anchor=(1.15, 0.5))\n",
    "fig.tight_layout()\n",
    "\n",
    "ax1.set(ylabel='Capture Time', xlabel='Prey Initial Position')\n",
    "ax2.set(ylabel='Percentage Captured', xlabel='Prey Initial Position')\n",
    "rescaled_axs = [str(num-(pk//2 + (width//2))) for num in range(pk//2, width+pk//2)]\n",
    "ax1.set_xticks(range(pk//2, width+pk//2), rescaled_axs);\n",
    "ax2.set_xticks(range(pk//2, width+pk//2), rescaled_axs);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding alpha for the memory based agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs noise\n",
    "noises = np.linspace(start=0.0, stop=2.0, num=13)\n",
    "policies = multimodal_mazes.AgentRuleBasedMemory.policies\n",
    "alphas = np.linspace(start=0.0, stop=2.0, num=11)\n",
    "results = np.zeros((len(noises), len(policies), len(alphas)))\n",
    "\n",
    "# Test agents\n",
    "for a, noise in enumerate(tqdm(noises)):\n",
    "\n",
    "    for b, policy in enumerate(policies): \n",
    "\n",
    "        for c, alpha in enumerate(alphas):\n",
    "            agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "            agnt.alpha=alpha\n",
    "            fitness, _, _, _ = multimodal_mazes.eval_predator_fitness(n_trials=n_trials, size=size, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, pm=pm, pe=pe)\n",
    "            results[a, b, c] = fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs noise\n",
    "colors = multimodal_mazes.AgentRuleBasedMemory.colors\n",
    "auc = np.trapz(y=results.T, x=noises, axis=2)\n",
    "for b, policy in enumerate(policies): \n",
    "    plt.plot(alphas, auc[:,b], color=colors[b], label=policy)\n",
    "\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring task parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs noise\n",
    "noises = np.linspace(start=0.0, stop=2.0, num=3)\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "pms = np.linspace(start=0.0, stop=1.0, num=3)\n",
    "pes = np.linspace(start=0.0, stop=1.0, num=3)\n",
    "\n",
    "results = np.zeros((len(noises), len(policies), len(pms), len(pes)))\n",
    "\n",
    "# Test agents\n",
    "for a, noise in enumerate(tqdm(noises)):\n",
    "\n",
    "    for b, policy in enumerate(policies): \n",
    "        if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "        elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "            agnt.alpha = 0.6\n",
    "        elif policy == \"Levy\":\n",
    "            agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "        for c, pm in enumerate(pms):\n",
    "            for d, pe in enumerate(pes):\n",
    "                fitness, _, _, _ = multimodal_mazes.eval_predator_fitness(n_trials=n_trials, size=size, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, motion=motion, pc=pc, pm=pm, pe=pe)\n",
    "\n",
    "                results[a, b, c, d] = fitness\n",
    "\n",
    "# np.save(\"results_\" + motion, results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results \n",
    "results = np.load(\"../results/test18/results.npy\")\n",
    "print(results.shape) # noises, policies, pms, pes \n",
    "\n",
    "parameters = np.load(\"../results/test18/parameters.npy\", allow_pickle=True)\n",
    "noises = parameters.item().get(\"noises\")\n",
    "policies = parameters.item().get(\"policies\")\n",
    "pms = parameters.item().get(\"pms\")\n",
    "pes = parameters.item().get(\"pes\")\n",
    "colors = parameters.item().get(\"colors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean AUC \n",
    "auc = np.zeros((len(policies), len(pms), len(pes)))\n",
    "for c, _ in enumerate(pms):\n",
    "    for d, _ in enumerate(pes):\n",
    "        auc[:,c,d] = np.trapz(y=results[:,:,c,d].T, x=noises, axis=1)\n",
    "\n",
    "for b, _ in enumerate(policies): \n",
    "    ml, sl, _ = plt.stem(b, np.mean(auc[b]) - np.mean(auc[0]))\n",
    "    ml.set_color(colors[b])\n",
    "    sl.set_color(colors[b])\n",
    "plt.xticks(range(len(policies)), policies, rotation='vertical')\n",
    "plt.ylabel('Normalised mean AUC');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference in AUC \n",
    "auc_diff = auc[-2,:,:] - auc[-5,:,:]\n",
    "print(auc_diff.min(), np.argwhere(auc_diff == np.min(auc_diff))) \n",
    "print(auc_diff.max(), np.argwhere(auc_diff == np.max(auc_diff))) \n",
    "\n",
    "for b, policy in enumerate(policies): \n",
    "    plt.plot(noises, results[:,\n",
    "        b, \n",
    "        np.argwhere(auc_diff == np.max(auc_diff))[0][0], \n",
    "        np.argwhere(auc_diff == np.max(auc_diff))[0][1]], \n",
    "        color=colors[b], \n",
    "        label=policy)\n",
    "\n",
    "plt.ylim([0, 1.05])\n",
    "plt.ylabel('Fitness')\n",
    "plt.xlabel('Sensor Noise')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat results (# noises, policies, pms, pes)\n",
    "results_arrays = [[] for _ in enumerate(policies)]\n",
    "for a, noise in enumerate(noises):\n",
    "    for b, policy in enumerate(policies):\n",
    "        for c, pm in enumerate(pms):\n",
    "            for d, pe in enumerate(pes):\n",
    "                results_arrays[b].append([noise, pm, pe, results[a, b, c, d]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve fits \n",
    "params = [\"Sensor noise\", \"$p_m$\", \"$p_e$\"]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(params), figsize=(15,5), sharex=False, sharey=True)\n",
    "\n",
    "for a, policy in enumerate(policies):\n",
    "    data = np.array(results_arrays[a])   \n",
    "\n",
    "    for b, param in enumerate(params):\n",
    "        plt.sca(ax[b])\n",
    "\n",
    "        # Data \n",
    "        x = data[:,b]\n",
    "        y = data[:,-1]\n",
    "        idx = np.argsort(x)\n",
    "\n",
    "        # Poly fit \n",
    "        curve = np.poly1d(np.polyfit(x[idx],y[idx],deg=2))\n",
    "        plt.plot(x[idx], curve(x[idx]), color=colors[a], label=policy)\n",
    "        \n",
    "        if (a == 0) and (b == 0): \n",
    "            plt.ylabel(\"Fitness\")\n",
    "        \n",
    "        if (a==0):\n",
    "            plt.xlabel(param)\n",
    "\n",
    "plt.ylim([0, 1.05])\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.concatenate((np.load(\"../results/test17/results_Brownian.npy\"), np.load(\"../results/test17/results_Levy.npy\")), axis=1)\n",
    "print(results.shape)\n",
    "\n",
    "auc = np.zeros((len(policies)*2, len(pms), len(pes)))\n",
    "for c, _ in enumerate(pms):\n",
    "    for d, _ in enumerate(pes):\n",
    "        auc[:,c,d] = np.trapz(y=results[:,:,c,d].T, x=noises, axis=1)\n",
    "\n",
    "for b, _ in enumerate(policies): \n",
    "    plt.scatter(np.mean(auc[b]), np.mean(auc[b + len(policies)]), color=colors[b])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy='Linear fusion')\n",
    "# time, path, prey_state, preys = multimodal_mazes.predator_trial(size=size, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk,n_steps=n_steps, scenario=scenario, pm=pm, pe=pe)\n",
    "time, path, prey_state, preys = multimodal_mazes.predator_trial(width=width, height=height, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk,n_steps=n_steps, scenario=scenario, pm=pm, pe=pe)\n",
    "print(prey_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "from matplotlib import colors\n",
    "prey_markers = ['P', 'X']\n",
    "\n",
    "# Environment \n",
    "pk_hw = pk // 2  # half width of prey's Gaussian signal (in rc)\n",
    "#env = np.zeros((size, size, len(agnt.channels) + 1))\n",
    "env = np.zeros((height, width, len(agnt.channels) + 1))\n",
    "env[:, :, -1] = 1.0\n",
    "env = np.pad(env, pad_width=((pk_hw, pk_hw), (pk_hw, pk_hw), (0, 0)))\n",
    "plt.imshow(1 - env[:, :, -1], cmap=\"binary\", alpha=0.25)\n",
    "\n",
    "# Path\n",
    "cmap = colors.LinearSegmentedColormap.from_list(\n",
    "    \"\", [\"xkcd:teal blue\", \"xkcd:off white\", \"xkcd:coral\"], N=n_steps\n",
    ")\n",
    "for t in range(len(path) - 1):\n",
    "    plt.plot([path[t, 1], path[t + 1, 1]], [path[t, 0], path[t + 1, 0]], c=cmap(t), zorder=0)\n",
    "    plt.scatter(path[t + 1, 1], path[t + 1, 0], s=30, color=cmap(t), zorder=1)\n",
    "\n",
    "# Prey \n",
    "for prey in preys:\n",
    "    path = np.array(prey.path)\n",
    "    if scenario == \"Foraging\":\n",
    "        plt.scatter(path[0,1], path[0,0], color='k', alpha=0.5, marker=prey_markers[prey.cues], zorder=2)\n",
    "    elif scenario == \"Hunting\":\n",
    "        plt.scatter(path[-1,1], path[-1,0], color='k', alpha=0.5, marker=prey_markers[0], zorder=2)\n",
    "\n",
    "# Adjust axes \n",
    "# plt.xlim([(pk//2) - 1, size + pk//2])\n",
    "# plt.ylim([size + pk//2, (pk//2) - 1]) \n",
    "plt.xlim([(pk//2) - 1, width + pk//2])\n",
    "plt.ylim([height + pk//2, (pk//2) - 1]) \n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIP: Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "visible_steps = n_steps\n",
    "pm = 0\n",
    "\n",
    "agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy='Nonlinear fusion')\n",
    "# time, path, prey_state, preys, env_log = multimodal_mazes.linear_prey_trial(width=width, height=height, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk,n_steps=n_steps, scenario=scenario, case=case, motion=motion, visible_steps=visible_steps, multisensory=multisensory, pc=pc, pm=pm, pe=pe, log_env=True)\n",
    "trial = multimodal_mazes.PredatorTrial(width=width, height=height, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk,n_steps=n_steps, scenario=scenario, case=case, motion=motion, visible_steps=visible_steps, multisensory=multisensory, pc=pc, pm=pm, pe=pe, log_env=True)\n",
    "time, path, prey_state, preys, env_log = trial.run_trial()\n",
    "print(prey_state)\n",
    "print(len(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = '2'\n",
    "pm = 0.8\n",
    "\n",
    "agnt = multimodal_mazes.AgentIntercept(location=None, channels=[1,1], policy='Kinetic alignment', direction=0)\n",
    "time, path, prey_state, preys, env_log = multimodal_mazes.linear_prey_trial(width=width, height=height, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk,n_steps=n_steps, scenario=scenario, case=case, motion=motion, visible_steps=visible_steps, multisensory=multisensory, pc=pc, pm=pm, pe=pe, log_env=True)\n",
    "print(prey_state)\n",
    "print(len(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable; using Pillow instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.animation as animation\n",
    "prey_markers = ['P', 'X']\n",
    "\n",
    "# Colormaps \n",
    "from matplotlib import colors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "cmap_wall = cm.binary\n",
    "cmap_wall.set_under('k', alpha=0)\n",
    "\n",
    "cmap_ch0 = colors.LinearSegmentedColormap.from_list(\n",
    "    \"\", [\"white\", \"xkcd:ultramarine\"]\n",
    ")\n",
    "\n",
    "cmap_ch1 = colors.LinearSegmentedColormap.from_list(\n",
    "    \"\", [\"white\", \"xkcd:magenta\"]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Environment \n",
    "plt.imshow(1 - env_log[0][:, :, -1], clim=[0.1,1.0], cmap=cmap_wall, alpha=0.25, zorder=1)\n",
    "plt.imshow((cmap_ch0(env_log[1][:,:,0]) + cmap_ch1(env_log[1][:,:,1]))/2, interpolation='gaussian', zorder=0) \n",
    "\n",
    "# Adjust axes \n",
    "plt.xlim([(pk//2) - 1, width + pk//2])\n",
    "plt.ylim([height + pk//2, (pk//2) - 1]) \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Initial data \n",
    "agnt_animation = ax.scatter([], [], s=120, color='k', zorder=3)\n",
    "preys_animation = [[] for _ in preys]\n",
    "for a, prey in enumerate(preys): \n",
    "    if scenario == \"Static\":\n",
    "     preys_animation[a] = ax.scatter([], [], s=60, color='k', alpha=0.5, marker=prey_markers[prey.cues], zorder=2)\n",
    "    elif scenario != \"Static\": \n",
    "        preys_animation[a] = ax.scatter([], [], s=60, color='k', alpha=0.5, marker=prey_markers[0], zorder=2)\n",
    "\n",
    "# Animate \n",
    "def update_animation(t):\n",
    "    plt.imshow((cmap_ch0(env_log[t][:,:,0]) + cmap_ch1(env_log[t][:,:,1]))/2, interpolation='gaussian', zorder=0) \n",
    "\n",
    "    agnt_animation.set_offsets([path[t, 1], path[t, 0]])\n",
    "\n",
    "    for a, prey in enumerate(preys): \n",
    "        try:\n",
    "            preys_animation[a].set_offsets([prey.path[t][1], prey.path[t][0]])\n",
    "        except:\n",
    "            preys_animation[a].set(alpha=0)\n",
    "\n",
    "#anim = animation.FuncAnimation(fig, update_animation, frames=range(0, len(path)), blit=False)\n",
    "anim = animation.FuncAnimation(fig, update_animation, frames=range(0, 20), blit=False)\n",
    "anim.save(\"Test.gif\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 channel colormap\n",
    "input_values = np.linspace(0,1,num=11)\n",
    "a,b = np.meshgrid(input_values, input_values)\n",
    "\n",
    "plt.imshow((cmap_ch0(a) + cmap_ch1(b))/2, zorder=0, origin='lower')\n",
    "plt.xticks(ticks=range(len(input_values)), labels=np.round(input_values,1), rotation='vertical')\n",
    "plt.yticks(ticks=range(len(input_values)), labels=np.round(input_values,1))\n",
    "plt.xlabel('Ch0 input')\n",
    "plt.ylabel('Ch1 input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness vs case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs case\n",
    "cases = [\"1\", \"2\", \"3\"]\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "results = np.zeros((len(cases), len(policies)))\n",
    "\n",
    "# Test agents\n",
    "for a, case in enumerate(cases):\n",
    "    for b, policy in enumerate(policies): \n",
    "        if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "        elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "            agnt.alpha = 0.6\n",
    "        elif policy == \"Levy\":\n",
    "            agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "        fitness, _, _, _, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=case, motion=motion, visible_steps=visible_steps, multisensory=multisensory, pc=pc, pm=pm, pe=pe)\n",
    "\n",
    "        results[a, b] = fitness\n",
    "\n",
    "for b, policy in enumerate(policies): \n",
    "    plt.plot(cases, results[:,b], color=colors[b], label=policy)\n",
    "\n",
    "plt.title(f\"{multisensory} Fitness vs Case\")\n",
    "plt.ylim([0, 1.05])\n",
    "plt.ylabel('Fitness')\n",
    "plt.xlabel('Case')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.4, 0.5), labels=policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage capture vs speed per case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Capture vs speed\n",
    "\n",
    "noise = 0.002\n",
    "pe = 0.998\n",
    "n_trials = 10\n",
    "cases = [\"1\", \"2\", \"3\"]\n",
    "speeds = np.linspace(start=0.0, stop=1.0, num=10)\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "results = np.zeros((len(speeds), len(policies)))\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "fig.tight_layout(rect=[0,0.1, 1, 0.9])\n",
    "fig.suptitle(f\"{motion}, {multisensory} - Percentage Captured vs Speed\")\n",
    "\n",
    "\n",
    "# Test agents\n",
    "for case in range(len(cases)):\n",
    "    for a, speed in enumerate(speeds):\n",
    "        for b, policy in enumerate(policies): \n",
    "            if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "                agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "            elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "                agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "                agnt.alpha = 0.6\n",
    "                \n",
    "            elif policy == \"Levy\":\n",
    "                agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "            _, _, _, _, captured, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=cases[case], motion=motion, visible_steps=visible_steps, multisensory=multisensory, pc=pc, pm=speeds[a], pe=pe)\n",
    "            results[a, b] = captured\n",
    "    \n",
    "\n",
    "    for b, policy in enumerate(policies): \n",
    "        axs[case].plot(speeds, results[:,b], color=colors[b])    \n",
    "        axs[case].set_title(f\"Case {case+1}\")\n",
    "\n",
    "axs[0].set(xlabel='Speed', ylabel='Percentage Captured')\n",
    "axs[0].set_xticks(np.arange(0.0, 1.1, 0.2));\n",
    "fig.legend(loc='center right', bbox_to_anchor=(1.05, 0.5), labels=policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Capture vs speed\n",
    "cases = [\"1\", \"2\", \"3\"]\n",
    "speeds = np.linspace(start=0.0, stop=1.0, num=10)\n",
    "repeats = [i for i in range(4)]\n",
    "n_trials = 10\n",
    "pe = 0.998\n",
    "noise = 0.002\n",
    "policy = multimodal_mazes.AgentRuleBased.policies[7]\n",
    "agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "# policy = multimodal_mazes.AgentRuleBasedMemory.policies[0]\n",
    "# agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "colors = [(\"#346ea8\"), (\"#ff760d\"), (\"#fa020f\")]\n",
    "results = np.zeros((len(speeds), len(repeats)))\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5), sharex=True, sharey=True)\n",
    "fig.tight_layout(rect=[0,0.1, 1, 0.9])\n",
    "fig.suptitle(f\"{multisensory} {policy} Percentage Captured vs Speed\")\n",
    "\n",
    "for case in range(len(cases)):\n",
    "    capture_results = np.zeros((2, len(speeds)))\n",
    "    for a, speed in enumerate(speeds):\n",
    "        \n",
    "        for b in repeats:\n",
    "            captured = 0\n",
    "            _, _, _, _, captured, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=cases[case], motion=motion, visible_steps=visible_steps, multisensory=multisensory, pc=pc, pm=speeds[a], pe=pe)\n",
    "            results[a, b] = captured\n",
    "        capture_results[0, a] = np.mean(results[a,:])\n",
    "        capture_results[1, a] = np.std(results[a,:])\n",
    "    \n",
    "    for b in repeats:\n",
    "        axs[case].plot(speeds, results[:,b], color = colors[case], alpha = 0.2)\n",
    "        axs[case].set_title(f\"Case {case+1}\")\n",
    "        axs[case].plot(speeds, capture_results[0, :], color=colors[case])\n",
    "        axs[3].plot(speeds, capture_results[0, :], color=colors[case])\n",
    "        axs[case].errorbar(speeds, capture_results[0, :], yerr=capture_results[1, :], color=colors[case])\n",
    "        axs[3].errorbar(speeds, capture_results[0, :], yerr=capture_results[1, :], color=colors[case])\n",
    "\n",
    "axs[0].set(xlabel='Speed', ylabel='Percentage Captured')\n",
    "axs[0].set_xticks(np.arange(0.0, 1.1, 0.2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disappearing percentage captured vs speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Captured vs speed\n",
    "n_trials = 50\n",
    "speeds = np.linspace(start=0.0, stop=1.0, num=10)\n",
    "visible_periods = [6, 5, 4, 3, 2, 1, 0]\n",
    "visible_periods_str = ['1', '2', '5', '10', '20', '50', '100']\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "results = np.zeros((len(speeds), len(policies)))\n",
    "aucs = np.zeros((len(visible_periods), len(policies)))\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 10), sharex=True, sharey=True)\n",
    "fig.tight_layout(rect=[0,0.1, 1, 0.9])\n",
    "\n",
    "# Test agents\n",
    "for c, period in enumerate(visible_periods):\n",
    "    actual_period = int(visible_periods_str[period])\n",
    "    for a, speed in enumerate(speeds):\n",
    "        for b, policy in enumerate(policies): \n",
    "            if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "                agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "            elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "                agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "                agnt.alpha = 0.6\n",
    "            elif policy == \"Levy\":\n",
    "                agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "            _, _, _, _, captured = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=case, motion=motion, visible_steps=actual_period, multisensory=multisensory, pc=pc, pm=speed, pe=pe)\n",
    "            results[a, b] = captured\n",
    "    \n",
    "    idy = 0\n",
    "    idx = c \n",
    "    auc = np.trapz(y=results.T, x=speeds, axis=1)\n",
    "    aucs[c] = auc\n",
    "    \n",
    "    if c >= 4:\n",
    "        idy = 1\n",
    "        idx = c - 4\n",
    "\n",
    "    for b, policy in enumerate(policies): \n",
    "        axs[idy][idx].plot(speeds, results[:,b], color=colors[b])    \n",
    "        axs[idy][idx].set_title(f\"Time to Disappear {actual_period}\")  \n",
    "    \n",
    "axs[0][0].set(xlabel='Speed', ylabel='Percentage Captured')\n",
    "axs[0][0].set_xticks(np.arange(0.0, 1.1, 0.2));\n",
    "fig.subplots_adjust(0, 0.05, 1, 0.95)\n",
    "fig.legend(loc='center right', bbox_to_anchor=(1.15, 0.5), labels=policies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b, policy in enumerate(policies): \n",
    "    plt.plot(visible_periods, aucs[:,b], color=colors[b], label=policy)\n",
    "\n",
    "plt.xticks(range(7), visible_periods_str);\n",
    "plt.xlim(6, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disappearing percentage capture vs visible time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Capture vs visible time steps\n",
    "n_trials = 50\n",
    "pm = 0.4\n",
    "cases = [\"1\", \"2\", \"3\"]\n",
    "visible_periods = [4, 3, 2, 1, 0]\n",
    "visible_periods_str = ['1', '5', '25', '50', '100']\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "results = np.zeros((len(visible_periods), len(policies)))\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "fig.tight_layout(rect=[0,0.1, 1, 0.9])\n",
    "fig.suptitle(f\"{multisensory} Fitness vs Time to Disappear\")\n",
    "\n",
    "# Test agents\n",
    "for c, case in enumerate(cases):\n",
    "    capture_results = np.zeros((2, len(visible_periods)))\n",
    "\n",
    "    for a, period in enumerate(visible_periods):\n",
    "        actual_period = int(visible_periods_str[period])\n",
    "\n",
    "        for b, policy in enumerate(policies): \n",
    "            if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "                agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "            elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "                agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "                agnt.alpha = 0.6\n",
    "            elif policy == \"Levy\":\n",
    "                agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "            _, _, _, _, captured, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=case, motion=motion, multisensory=multisensory, visible_steps=actual_period, pc=pc, pm=pm, pe=pe)\n",
    "            \n",
    "            results[a, b] = captured\n",
    "        #capture_results[0, a] = np.mean(results[a,:])\n",
    "        #capture_results[1, a] = np.std(results[a,:])\n",
    "   \n",
    "    for b, policy in enumerate(policies): \n",
    "        axs[c].plot(visible_periods, results[:,b], color=colors[b])    \n",
    "        axs[c].set_title(f\"Case {case}\")\n",
    "\n",
    "axs[0].set(xlabel='Time to Disappear', ylabel='Percentage Captured', )\n",
    "axs[0].set_xticks(range(5), visible_periods_str);\n",
    "# axs[0].xaxis.set_inverted(True)\n",
    "fig.legend(loc='center right', bbox_to_anchor=(1.05, 0.5), labels=policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Captured vs visible time steps poster scaling\n",
    "speed = 0.4\n",
    "repeats = [i for i in range(1)]\n",
    "cases = [\"1\", \"2\", \"3\"]\n",
    "visible_periods = [4, 3, 2, 1, 0]\n",
    "visible_periods_str = ['1', '5', '25', '50', '100']\n",
    "n_trials = 50\n",
    "# policy = multimodal_mazes.AgentRuleBased.policies[7]\n",
    "# agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "policy = multimodal_mazes.AgentRuleBasedMemory.policies[0]\n",
    "agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "colors = [(\"#346ea8\"), (\"#ff760d\"), (\"#fa020f\")]\n",
    "results = np.zeros((len(visible_periods), len(repeats))))\n",
    "\n",
    "# Test agents\n",
    "for a, case in enumerate(cases):\n",
    "    capture_results = np.zeros((2, len(visible_periods)))\n",
    "    for b, period in enumerate(visible_periods):\n",
    "        actual_period = int(visible_periods_str[period])\n",
    "        \n",
    "        for c in repeats:\n",
    "            _, _, _, _, captured, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=case, motion=motion, visible_steps=actual_period, multisensory=multisensory, pc=pc, pm=speed, pe=pe)\n",
    "            results[b, c] = captured\n",
    "        capture_results[0, b] = np.mean(results[b,:])\n",
    "        capture_results[1, b] = np.std(results[b,:])\n",
    "\n",
    "    for c in repeats:\n",
    "        plt.plot(visible_periods, results[:,c], color=colors[a], alpha=0.2)    \n",
    "    plt.errorbar(visible_periods, capture_results[0, :], yerr=capture_results[1, :], color=colors[a])\n",
    "\n",
    "plt.title(f\"{multisensory} {policy} Percentage Captured vs Time to Disappear\")  \n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Time to Disappear\")\n",
    "plt.ylabel(\"Percentage Captured\")\n",
    "# plt.xlim([4, 0])\n",
    "plt.xticks(range(5), visible_periods_str);\n",
    "plt.yticks(np.arange(0, 101, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Captured vs visible time steps poster scaling\n",
    "\n",
    "speeds = [0, 0.25, 0.5, 0.75]\n",
    "repeats = [i for i in range(4)]\n",
    "cases = [\"1\", \"2\", \"3\"]\n",
    "visible_periods = [3, 2, 1, 0]\n",
    "visible_periods_str = ['1', '10', '15', '25']\n",
    "n_trials = 10\n",
    "# policy = multimodal_mazes.AgentRuleBased.policies[7]\n",
    "# agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "policy = multimodal_mazes.AgentRuleBasedMemory.policies[1]\n",
    "agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "colors = [(\"#346ea8\"), (\"#ff760d\"), (\"#fa020f\")]\n",
    "results = np.zeros((len(visible_periods), len(repeats)))\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5), sharex=True, sharey=True)\n",
    "fig.tight_layout(rect=[0,0.1, 1, 0.9])\n",
    "\n",
    "# Test agents\n",
    "case_results = np.zeros((len(speeds), len(cases), len(visible_periods)))\n",
    "for d, speed in enumerate(speeds):\n",
    "    for a, case in enumerate(cases):\n",
    "        capture_results = np.zeros((2, len(visible_periods)))\n",
    "\n",
    "        for b, period in enumerate(visible_periods):\n",
    "            actual_period = int(visible_periods_str[period])\n",
    "            \n",
    "            for c in repeats:\n",
    "                _, _, _, _, captured, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=case, motion=motion, visible_steps=actual_period, multisensory=multisensory, pc=pc, pm=speed, pe=pe)\n",
    "                results[b, c] = captured\n",
    "                \n",
    "            capture_results[0, b] = np.mean(results[b,:])\n",
    "            capture_results[1, b] = np.std(results[b,:])\n",
    "            \n",
    "        case_results[d, a] = capture_results[0]\n",
    "\n",
    "        for c in repeats:\n",
    "            axs[d].plot(visible_periods, results[:,c], color=colors[a], alpha=0.2)    \n",
    "            axs[d].set_title(f\"Speed {speed}\")  \n",
    "\n",
    "        axs[d].errorbar(visible_periods, capture_results[0, :], yerr=capture_results[1, :], color=colors[a])\n",
    "\n",
    "averaged_case_results = np.mean(case_results, axis=0)\n",
    "        \n",
    "axs[0].set(xlabel='Time to Disapepar', ylabel='Percentage Captured')\n",
    "axs[0].set_xticks(range(4), visible_periods_str);\n",
    "fig.suptitle(f\"{multisensory} {policy} Percentage Captured vs Time to Disappear\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, case in enumerate(averaged_case_results):\n",
    "    plt.plot(visible_periods, case, color=colors[a])\n",
    "\n",
    "plt.xlim(3, 0)\n",
    "plt.xticks(range(4), visible_periods_str);\n",
    "plt.title('Percentage Captured vs Time to Disappear')\n",
    "plt.xlabel('Time to Disappear')\n",
    "plt.ylabel('Percentage Captured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Approach vs visible time steps poster scaling\n",
    "speed = 0.3\n",
    "repeats = [i for i in range(4)]\n",
    "cases = [\"1\", \"2\", \"3\"]\n",
    "visible_periods = [5, 4, 3, 2, 1, 0]\n",
    "visible_periods_str = ['4', '6', '8', '10', '12', '14']\n",
    "n_trials = 100\n",
    "# policy = multimodal_mazes.AgentRuleBased.policies[7]\n",
    "# agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "policy = multimodal_mazes.AgentRuleBasedMemory.policies[0]\n",
    "agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "colors = [(\"#346ea8\"), (\"#ff760d\"), (\"#fa020f\")]\n",
    "results = np.zeros((len(visible_periods), len(repeats)))\n",
    "\n",
    "# Test agents\n",
    "for a, case in enumerate(cases):\n",
    "    approach_results = np.zeros((2, len(visible_periods)))\n",
    "    for b, period in enumerate(visible_periods):\n",
    "        actual_period = int(visible_periods_str[period])\n",
    "        \n",
    "        for c in repeats:\n",
    "            _, _, _, _, _, approached = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=case, motion=motion, visible_steps=actual_period, multisensory=multisensory, pc=pc, pm=speed, pe=pe)\n",
    "            results[b, c] = approached\n",
    "        approach_results[0, b] = np.mean(results[b,:])\n",
    "        approach_results[1, b] = np.std(results[b,:])\n",
    "\n",
    "    for c in repeats:\n",
    "        plt.plot(visible_periods, results[:,c], color=colors[a], alpha=0.2)    \n",
    "    plt.errorbar(visible_periods, approach_results[0, :], yerr=approach_results[1, :], color=colors[a])\n",
    "\n",
    "plt.title(f\"{multisensory} {policy} Percentage Approached vs Time to Disappear\")  \n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Time to Disappear\")\n",
    "plt.ylabel(\"Approached Captured\")\n",
    "plt.xlim([5, 0])\n",
    "plt.xticks(range(6), visible_periods_str);\n",
    "plt.yticks(np.arange(0, 101, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two prey capture probabilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two prey capture probability vs noise\n",
    "n_trials = 100\n",
    "pm = 0\n",
    "noises = np.linspace(start=0.0, stop=1.0, num=10)\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "results = np.zeros((len(noises), len(policies)))\n",
    "\n",
    "for a, noise in enumerate(noises):\n",
    "    for b, policy in enumerate(policies): \n",
    "        if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "        elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "            agnt.alpha = 0.6\n",
    "        elif policy == \"Levy\":\n",
    "            agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "        _, _, _, preys, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=case, visible_steps=visible_steps, multisensory=multisensory, motion=motion, pc=pc, pm=pm, pe=pe)\n",
    "        \n",
    "        ms_captured = 0\n",
    "\n",
    "        for prey in preys:\n",
    "            if prey[0].state == 0:\n",
    "                ms_captured += 1\n",
    "        ms_captured = (ms_captured / n_trials)\n",
    "        results[a, b] = ms_captured\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for b, policy in enumerate(policies): \n",
    "    plt.plot(noises, results[:,b], color=colors[b], label=policy)\n",
    "plt.ylabel('Probability of Multisensory Prey Caught First')\n",
    "plt.xlabel('Noise')\n",
    "plt.yticks(np.arange(0.0, 1.1, 0.1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two prey capture probability\n",
    "n_trials = 100\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "results = np.zeros((len(policies)))\n",
    "\n",
    "for b, policy in enumerate(policies): \n",
    "    if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "        agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "    elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "        agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "        agnt.alpha = 0.6\n",
    "    elif policy == \"Levy\":\n",
    "        agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "    _, _, _, preys, _, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=case, visible_steps=visible_steps, multisensory=multisensory, motion=motion, pc=pc, pm=pm, pe=pe)\n",
    "    \n",
    "    ms_captured = 0\n",
    "\n",
    "    for prey in preys:\n",
    "        if prey[0].state == 0:\n",
    "            ms_captured += 1\n",
    "    ms_captured = (ms_captured / n_trials)\n",
    "    results[b] = ms_captured\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for b, policy in enumerate(policies):\n",
    "    plt.scatter(policy, results[b], color=colors[b])\n",
    "    ml, sl, _ = plt.stem(b, results[b])\n",
    "    ml.set_color(colors[b])\n",
    "    sl.set_color(colors[b])\n",
    "plt.ylabel('Probability of Multisensory Prey Caught First')\n",
    "plt.xlabel('Policy')\n",
    "plt.xticks(range(len(policies)), policies, rotation='vertical');\n",
    "plt.yticks(np.arange(0.0, 1.1, 0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing kinetic alignment intercept  agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = ['1', '2', '3']\n",
    "speeds = np.linspace(start=0.1, stop=1.0, num=10)\n",
    "n_trials = 250\n",
    "policy='Kinetic alignment'\n",
    "colors = [(\"#346ea8\"), (\"#ff760d\"), (\"#fa020f\")]\n",
    "agnt = multimodal_mazes.AgentIntercept(location=None, channels=[1,1], policy='Kinetic alignment', direction=0)\n",
    "results = np.zeros((len(speeds)))\n",
    "\n",
    "# Test agents\n",
    "for c, case in enumerate(cases):\n",
    "    for a, speed in enumerate(speeds):\n",
    "        _, _, _, _, captured, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=case, motion=motion, multisensory=multisensory, visible_steps=visible_steps, pc=pc, pm=speed, pe=pe)\n",
    "        results[a] = captured      \n",
    "            \n",
    "    for a, speed in enumerate(speeds):\n",
    "        plt.plot(speeds, results[:], color=colors[c])\n",
    "\n",
    "plt.title(f\"{multisensory} {policy} Percentage Captured vs Case\")  \n",
    "plt.xlabel(\"Speed\")\n",
    "plt.ylabel(\"Percentage Captured\")\n",
    "plt.xticks(np.arange(0.0, 1.1, 0.2));\n",
    "#plt.yticks(np.arange(0, 101, 10));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multimodal_mazes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
