{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prey notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import neat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import multimodal_mazes\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas \n",
    "\n",
    "* Odour - constant but noisy. \n",
    "* Sound - reliable but infrequent.\n",
    "* Rather than resetting the env to zero every step, you could decay it. E.g. env[:,:,:-1] *= 0.8 + blur. \n",
    "* Analysis: n_prey caught, speed, costs (e.g. movement vs food). \n",
    "* Evolve prey against different algorithms. Then, evolve predators against these prey.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "width=31\n",
    "height=11\n",
    "n_prey = 1\n",
    "n_steps = 50\n",
    "n_trials = 100\n",
    "pk = 50 # the width of the prey's Gaussian signal (in rc)\n",
    "visible_steps = n_steps\n",
    "scenario = \"Constant\"\n",
    "motion = \"Linear\"\n",
    "multisensory = \"Unisensory\"\n",
    "\n",
    "if scenario == \"Static\":\n",
    "    pc = 0.0\n",
    "    pm = None\n",
    "    pe = None \n",
    "    motion = None\n",
    "elif scenario == \"Constant\" and motion == \"Linear\":\n",
    "    pc = 0.0\n",
    "    pm = 0.5\n",
    "    pe = 0.2\n",
    "    case = \"2\"\n",
    "elif scenario == \"Constant\" and motion == \"Disappearing\":\n",
    "    pc = 0.0\n",
    "    pm = 0.5\n",
    "    pe = 0.2\n",
    "    visible_steps = 3\n",
    "    case = \"2\"\n",
    "elif scenario == \"Random\":\n",
    "    pc = 0.0\n",
    "    pm = 1\n",
    "    pe = 0.2\n",
    "    motion = \"Levy\"\n",
    "    case = \"1\"\n",
    "elif scenario == \"Two Prey\":\n",
    "    pc = 0.0\n",
    "    pm = 0.2\n",
    "    pe = 0.2\n",
    "    motion = \"Linear\"\n",
    "    multisensory = \"Balanced\"\n",
    "    case = \"4\"\n",
    "    n_prey = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness vs noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs noise\n",
    "noises = np.linspace(start=0.0, stop=2.0, num=10)\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "results = np.zeros((len(noises), len(policies)))\n",
    "\n",
    "# Test agents\n",
    "for a, noise in enumerate(tqdm(noises)):\n",
    "\n",
    "    for b, policy in enumerate(policies): \n",
    "        if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "        elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "            agnt.alpha = 0.6\n",
    "        elif policy == \"Levy\":\n",
    "            agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "        fitness, _, _, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=\"2 L\", motion=motion, pc=pc, pm=pm, pe=pe)\n",
    "\n",
    "        results[a, b] = fitness\n",
    "\n",
    "for b, policy in enumerate(policies): \n",
    "    plt.plot(noises, results[:,b], color=colors[b], label=policy)\n",
    "\n",
    "plt.ylim([0, 1.05])\n",
    "plt.ylabel('Fitness')\n",
    "plt.xlabel('Sensor Noise')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs noise AUC \n",
    "auc = np.trapz(y=results.T, x=noises, axis=1)\n",
    "for b, _ in enumerate(policies): \n",
    "    ml, sl, _ = plt.stem(b, auc[b] - auc[0])\n",
    "    ml.set_color(colors[b])\n",
    "    sl.set_color(colors[b])\n",
    "plt.xticks(range(len(policies)), policies, rotation='vertical')\n",
    "plt.ylabel('AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prey initial position vs capture time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prey initial position vs capture time\n",
    "noise = 0\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for b, policy in enumerate(policies): \n",
    "    if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "        agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "    elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "        agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "        agnt.alpha = 0.6\n",
    "    elif policy == \"Levy\":\n",
    "        agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "    _, times, _, preys = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=\"\", motion=motion, pc=pc, pm=pm, pe=pe) \n",
    "\n",
    "    percentage_results = []\n",
    "    capture_results = {str(key):[0,0] for key in range(pk//2, width+pk//2)}\n",
    "    \n",
    "    for prey in preys:\n",
    "        if prey[0].state == 0:\n",
    "            capture_results[str(prey[0].path[0][1])][1] += 1\n",
    "        capture_results[str(prey[0].path[0][1])][0] += 1\n",
    "    \n",
    "    for key,value in capture_results.items():\n",
    "        percentage = 0\n",
    "        if value[0] != 0:\n",
    "            percentage = (value[1]/value[0])*100\n",
    "        percentage_results.append([int(key), percentage])\n",
    "    \n",
    "    percentage_results = sorted(percentage_results)\n",
    "    percentage_positions = [percentage_results[it][0] for it in range(len(percentage_results))]\n",
    "    percentages = [percentage_results[it][1] for it in range(len(percentage_results))]\n",
    "\n",
    "    time_results = sorted([[preys[result][0].path[0][1], times[result]] for result in range(len(preys))]) # uncaptured prey have time set to maximum\n",
    "    time_positions = [time_results[it][0] for it in range(len(time_results))]\n",
    "    sorted_times = [time_results[it][1] for it in range(len(time_results))]\n",
    "    \n",
    "    curve1 = np.poly1d(np.polyfit(time_positions, sorted_times, deg=2))\n",
    "    y1 = curve1(time_positions) \n",
    "    y1[y1 <= 2] = 2\n",
    "    ax1.plot(time_positions, y1, color=colors[b], label=policies[b])\n",
    "\n",
    "    curve2 = np.poly1d(np.polyfit(percentage_positions, percentages, deg=2))\n",
    "    y2 = curve2(percentage_positions) \n",
    "    y2[y2 > 100] = 100\n",
    "    y2[y2 < 0] = 0\n",
    "    ax2.plot(percentage_positions, y2, color=colors[b])\n",
    "\n",
    "fig.suptitle(\"Preys Inital Position Correlation Capture Time and Percentage Caught\")\n",
    "fig.legend(loc='center right', bbox_to_anchor=(1.15, 0.5))\n",
    "fig.tight_layout()\n",
    "\n",
    "ax1.set(ylabel='Capture Time', xlabel='Prey Initial Position')\n",
    "ax2.set(ylabel='Percentage Captured', xlabel='Prey Initial Position')\n",
    "rescaled_axs = [str(num-(pk//2 + (width//2))) for num in range(pk//2, width+pk//2)]\n",
    "ax1.set_xticks(range(pk//2, width+pk//2), rescaled_axs);\n",
    "ax2.set_xticks(range(pk//2, width+pk//2), rescaled_axs);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding alpha for the memory based agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs noise\n",
    "noises = np.linspace(start=0.0, stop=2.0, num=13)\n",
    "policies = multimodal_mazes.AgentRuleBasedMemory.policies\n",
    "alphas = np.linspace(start=0.0, stop=2.0, num=11)\n",
    "results = np.zeros((len(noises), len(policies), len(alphas)))\n",
    "\n",
    "# Test agents\n",
    "for a, noise in enumerate(tqdm(noises)):\n",
    "\n",
    "    for b, policy in enumerate(policies): \n",
    "\n",
    "        for c, alpha in enumerate(alphas):\n",
    "            agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "            agnt.alpha=alpha\n",
    "            fitness, _, _, _ = multimodal_mazes.eval_predator_fitness(n_trials=n_trials, size=size, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, pm=pm, pe=pe)\n",
    "            results[a, b, c] = fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs noise\n",
    "colors = multimodal_mazes.AgentRuleBasedMemory.colors\n",
    "auc = np.trapz(y=results.T, x=noises, axis=2)\n",
    "for b, policy in enumerate(policies): \n",
    "    plt.plot(alphas, auc[:,b], color=colors[b], label=policy)\n",
    "\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring task parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs noise\n",
    "noises = np.linspace(start=0.0, stop=2.0, num=3)\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "pms = np.linspace(start=0.0, stop=1.0, num=3)\n",
    "pes = np.linspace(start=0.0, stop=1.0, num=3)\n",
    "\n",
    "results = np.zeros((len(noises), len(policies), len(pms), len(pes)))\n",
    "\n",
    "# Test agents\n",
    "for a, noise in enumerate(tqdm(noises)):\n",
    "\n",
    "    for b, policy in enumerate(policies): \n",
    "        if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "        elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "            agnt.alpha = 0.6\n",
    "        elif policy == \"Levy\":\n",
    "            agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "        for c, pm in enumerate(pms):\n",
    "            for d, pe in enumerate(pes):\n",
    "                fitness, _, _, _ = multimodal_mazes.eval_predator_fitness(n_trials=n_trials, size=size, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, motion=motion, pc=pc, pm=pm, pe=pe)\n",
    "\n",
    "                results[a, b, c, d] = fitness\n",
    "\n",
    "# np.save(\"results_\" + motion, results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results \n",
    "results = np.load(\"../results/test18/results.npy\")\n",
    "print(results.shape) # noises, policies, pms, pes \n",
    "\n",
    "parameters = np.load(\"../results/test18/parameters.npy\", allow_pickle=True)\n",
    "noises = parameters.item().get(\"noises\")\n",
    "policies = parameters.item().get(\"policies\")\n",
    "pms = parameters.item().get(\"pms\")\n",
    "pes = parameters.item().get(\"pes\")\n",
    "colors = parameters.item().get(\"colors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean AUC \n",
    "auc = np.zeros((len(policies), len(pms), len(pes)))\n",
    "for c, _ in enumerate(pms):\n",
    "    for d, _ in enumerate(pes):\n",
    "        auc[:,c,d] = np.trapz(y=results[:,:,c,d].T, x=noises, axis=1)\n",
    "\n",
    "for b, _ in enumerate(policies): \n",
    "    ml, sl, _ = plt.stem(b, np.mean(auc[b]) - np.mean(auc[0]))\n",
    "    ml.set_color(colors[b])\n",
    "    sl.set_color(colors[b])\n",
    "plt.xticks(range(len(policies)), policies, rotation='vertical')\n",
    "plt.ylabel('Normalised mean AUC');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference in AUC \n",
    "auc_diff = auc[-2,:,:] - auc[-5,:,:]\n",
    "print(auc_diff.min(), np.argwhere(auc_diff == np.min(auc_diff))) \n",
    "print(auc_diff.max(), np.argwhere(auc_diff == np.max(auc_diff))) \n",
    "\n",
    "for b, policy in enumerate(policies): \n",
    "    plt.plot(noises, results[:,\n",
    "        b, \n",
    "        np.argwhere(auc_diff == np.max(auc_diff))[0][0], \n",
    "        np.argwhere(auc_diff == np.max(auc_diff))[0][1]], \n",
    "        color=colors[b], \n",
    "        label=policy)\n",
    "\n",
    "plt.ylim([0, 1.05])\n",
    "plt.ylabel('Fitness')\n",
    "plt.xlabel('Sensor Noise')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat results (# noises, policies, pms, pes)\n",
    "results_arrays = [[] for _ in enumerate(policies)]\n",
    "for a, noise in enumerate(noises):\n",
    "    for b, policy in enumerate(policies):\n",
    "        for c, pm in enumerate(pms):\n",
    "            for d, pe in enumerate(pes):\n",
    "                results_arrays[b].append([noise, pm, pe, results[a, b, c, d]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve fits \n",
    "params = [\"Sensor noise\", \"$p_m$\", \"$p_e$\"]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(params), figsize=(15,5), sharex=False, sharey=True)\n",
    "\n",
    "for a, policy in enumerate(policies):\n",
    "    data = np.array(results_arrays[a])   \n",
    "\n",
    "    for b, param in enumerate(params):\n",
    "        plt.sca(ax[b])\n",
    "\n",
    "        # Data \n",
    "        x = data[:,b]\n",
    "        y = data[:,-1]\n",
    "        idx = np.argsort(x)\n",
    "\n",
    "        # Poly fit \n",
    "        curve = np.poly1d(np.polyfit(x[idx],y[idx],deg=2))\n",
    "        plt.plot(x[idx], curve(x[idx]), color=colors[a], label=policy)\n",
    "        \n",
    "        if (a == 0) and (b == 0): \n",
    "            plt.ylabel(\"Fitness\")\n",
    "        \n",
    "        if (a==0):\n",
    "            plt.xlabel(param)\n",
    "\n",
    "plt.ylim([0, 1.05])\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.concatenate((np.load(\"../results/test17/results_Brownian.npy\"), np.load(\"../results/test17/results_Levy.npy\")), axis=1)\n",
    "print(results.shape)\n",
    "\n",
    "auc = np.zeros((len(policies)*2, len(pms), len(pes)))\n",
    "for c, _ in enumerate(pms):\n",
    "    for d, _ in enumerate(pes):\n",
    "        auc[:,c,d] = np.trapz(y=results[:,:,c,d].T, x=noises, axis=1)\n",
    "\n",
    "for b, _ in enumerate(policies): \n",
    "    plt.scatter(np.mean(auc[b]), np.mean(auc[b + len(policies)]), color=colors[b])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy='Linear fusion')\n",
    "noise = 0.1\n",
    "# time, path, prey_state, preys = multimodal_mazes.predator_trial(size=size, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk,n_steps=n_steps, scenario=scenario, pm=pm, pe=pe)\n",
    "time, path, prey_state, preys = multimodal_mazes.predator_trial(width=width, height=height, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk,n_steps=n_steps, scenario=scenario, pm=pm, pe=pe)\n",
    "print(prey_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "from matplotlib import colors\n",
    "prey_markers = ['P', 'X']\n",
    "\n",
    "# Environment \n",
    "pk_hw = pk // 2  # half width of prey's Gaussian signal (in rc)\n",
    "#env = np.zeros((size, size, len(agnt.channels) + 1))\n",
    "env = np.zeros((height, width, len(agnt.channels) + 1))\n",
    "env[:, :, -1] = 1.0\n",
    "env = np.pad(env, pad_width=((pk_hw, pk_hw), (pk_hw, pk_hw), (0, 0)))\n",
    "plt.imshow(1 - env[:, :, -1], cmap=\"binary\", alpha=0.25)\n",
    "\n",
    "# Path\n",
    "cmap = colors.LinearSegmentedColormap.from_list(\n",
    "    \"\", [\"xkcd:teal blue\", \"xkcd:off white\", \"xkcd:coral\"], N=n_steps\n",
    ")\n",
    "for t in range(len(path) - 1):\n",
    "    plt.plot([path[t, 1], path[t + 1, 1]], [path[t, 0], path[t + 1, 0]], c=cmap(t), zorder=0)\n",
    "    plt.scatter(path[t + 1, 1], path[t + 1, 0], s=30, color=cmap(t), zorder=1)\n",
    "\n",
    "# Prey \n",
    "for prey in preys:\n",
    "    path = np.array(prey.path)\n",
    "    if scenario == \"Foraging\":\n",
    "        plt.scatter(path[0,1], path[0,0], color='k', alpha=0.5, marker=prey_markers[prey.cues], zorder=2)\n",
    "    elif scenario == \"Hunting\":\n",
    "        plt.scatter(path[-1,1], path[-1,0], color='k', alpha=0.5, marker=prey_markers[0], zorder=2)\n",
    "\n",
    "# Adjust axes \n",
    "# plt.xlim([(pk//2) - 1, size + pk//2])\n",
    "# plt.ylim([size + pk//2, (pk//2) - 1]) \n",
    "plt.xlim([(pk//2) - 1, width + pk//2])\n",
    "plt.ylim([height + pk//2, (pk//2) - 1]) \n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIP: Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy='Linear fusion')\n",
    "noise = 0\n",
    "time, path, prey_state, preys, env_log = multimodal_mazes.linear_prey_trial(width=width, height=height, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk,n_steps=n_steps, scenario=scenario, case=case, motion=motion, visible_steps=visible_steps, multisensory=multisensory, pc=pc, pm=pm, pe=pe, log_env=True)\n",
    "print(prey_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "prey_markers = ['P', 'X']\n",
    "\n",
    "# Colormaps \n",
    "from matplotlib import colors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "cmap_wall = cm.binary\n",
    "cmap_wall.set_under('k', alpha=0)\n",
    "\n",
    "cmap_ch0 = colors.LinearSegmentedColormap.from_list(\n",
    "    \"\", [\"white\", \"xkcd:ultramarine\"]\n",
    ")\n",
    "\n",
    "cmap_ch1 = colors.LinearSegmentedColormap.from_list(\n",
    "    \"\", [\"white\", \"xkcd:magenta\"]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Environment \n",
    "plt.imshow(1 - env_log[0][:, :, -1], clim=[0.1,1.0], cmap=cmap_wall, alpha=0.25, zorder=1)\n",
    "plt.imshow((cmap_ch0(env_log[1][:,:,0]) + cmap_ch1(env_log[1][:,:,1]))/2, interpolation='gaussian', zorder=0) \n",
    "\n",
    "# Adjust axes \n",
    "plt.xlim([(pk//2) - 1, width + pk//2])\n",
    "plt.ylim([height + pk//2, (pk//2) - 1]) \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Initial data \n",
    "agnt_animation = ax.scatter([], [], s=120, color='k', zorder=3)\n",
    "preys_animation = [[] for _ in preys]\n",
    "for a, prey in enumerate(preys): \n",
    "    if scenario == \"Static\":\n",
    "     preys_animation[a] = ax.scatter([], [], s=60, color='k', alpha=0.5, marker=prey_markers[prey.cues], zorder=2)\n",
    "    elif scenario != \"Static\": \n",
    "        preys_animation[a] = ax.scatter([], [], s=60, color='k', alpha=0.5, marker=prey_markers[0], zorder=2)\n",
    "\n",
    "# Animate \n",
    "def update_animation(t):\n",
    "    plt.imshow((cmap_ch0(env_log[t][:,:,0]) + cmap_ch1(env_log[t][:,:,1]))/2, interpolation='gaussian', zorder=0) \n",
    "\n",
    "    agnt_animation.set_offsets([path[t, 1], path[t, 0]])\n",
    "\n",
    "    for a, prey in enumerate(preys): \n",
    "        try:\n",
    "            preys_animation[a].set_offsets([prey.path[t][1], prey.path[t][0]])\n",
    "        except:\n",
    "            preys_animation[a].set(alpha=0)\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update_animation, frames=range(0, len(path)), blit=False)\n",
    "anim.save(\"Test.gif\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 channel colormap\n",
    "input_values = np.linspace(0,1,num=11)\n",
    "a,b = np.meshgrid(input_values, input_values)\n",
    "\n",
    "plt.imshow((cmap_ch0(a) + cmap_ch1(b))/2, zorder=0, origin='lower')\n",
    "plt.xticks(ticks=range(len(input_values)), labels=np.round(input_values,1), rotation='vertical')\n",
    "plt.yticks(ticks=range(len(input_values)), labels=np.round(input_values,1))\n",
    "plt.xlabel('Ch0 input')\n",
    "plt.ylabel('Ch1 input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness vs case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs case\n",
    "noise = 0\n",
    "cases = [\"1\", \"2\", \"3\"]\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "results = np.zeros((len(cases), len(policies)))\n",
    "\n",
    "# Test agents\n",
    "for a, case in enumerate(cases):\n",
    "    for b, policy in enumerate(policies): \n",
    "        if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "        elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "            agnt.alpha = 0.6\n",
    "        elif policy == \"Levy\":\n",
    "            agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "        fitness, _, _, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=cases[a], motion=motion, visible_steps=visible_steps, multisensory=multisensory, pc=pc, pm=pm, pe=pe)\n",
    "\n",
    "        results[a, b] = fitness\n",
    "\n",
    "for b, policy in enumerate(policies): \n",
    "    plt.plot(cases, results[:,b], color=colors[b], label=policy)\n",
    "\n",
    "plt.title(f\"{motion}, {multisensory} - Fitness vs Case\")\n",
    "plt.ylim([0, 1.05])\n",
    "plt.ylabel('Fitness')\n",
    "plt.xlabel('Case')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.4, 0.5), labels=policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness vs speed per case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs speed\n",
    "noise = 0\n",
    "cases = [\"1\", \"2\", \"3\"]\n",
    "speeds = np.linspace(start=0.0, stop=1.0, num=10)\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "results = np.zeros((len(speeds), len(policies)))\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "fig.tight_layout(rect=[0,0.1, 1, 0.9])\n",
    "fig.suptitle(f\"{motion}, {multisensory} - Fitness vs Speed\")\n",
    "\n",
    "\n",
    "# Test agents\n",
    "for case in range(len(cases)):\n",
    "    for a, speed in enumerate(speeds):\n",
    "        for b, policy in enumerate(policies): \n",
    "            if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "                agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "            elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "                agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "                agnt.alpha = 0.6\n",
    "                \n",
    "            elif policy == \"Levy\":\n",
    "                agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "            fitness, _, _, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=cases[case], motion=motion, visible_steps=visible_steps, multisensory=multisensory, pc=pc, pm=speeds[a], pe=pe)\n",
    "            results[a, b] = fitness\n",
    "    \n",
    "\n",
    "    for b, policy in enumerate(policies): \n",
    "        axs[case].plot(speeds, results[:,b], color=colors[b])    \n",
    "        axs[case].set_title(f\"Case {case+1}\")\n",
    "\n",
    "axs[0].set(xlabel='Speed', ylabel='Fitness')\n",
    "axs[0].set_xticks(np.arange(0.0, 1.1, 0.2));\n",
    "fig.legend(loc='center right', bbox_to_anchor=(1.05, 0.5), labels=policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs speed\n",
    "noise = 0\n",
    "repeats = [i for i in range(8)]\n",
    "cases = [\"1\", \"2\", \"3\"]\n",
    "speeds = np.linspace(start=0.0, stop=1.0, num=10)\n",
    "policy = multimodal_mazes.AgentRuleBased.policies[7]\n",
    "agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "colors = [(\"#346ea8\"), (\"#ff760d\"), (\"#fa020f\")]\n",
    "results = np.zeros((len(speeds), len(repeats)))\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5), sharex=True, sharey=True)\n",
    "fig.tight_layout(rect=[0,0.1, 1, 0.9])\n",
    "fig.suptitle(f\"{motion}, {multisensory} - {policy} Fitness vs Speed\")\n",
    "\n",
    "for case in range(len(cases)):\n",
    "    mean_fitness = []\n",
    "    for a, speed in enumerate(speeds):\n",
    "        sum_fitness = 0\n",
    "        for b in repeats:\n",
    "            fitness, _, _, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=cases[case], motion=motion, visible_steps=visible_steps, multisensory=multisensory, pc=pc, pm=speeds[a], pe=pe)\n",
    "            \n",
    "            results[a, b] = fitness\n",
    "            sum_fitness+=fitness\n",
    "        if sum_fitness == 0:\n",
    "            mean_fitness.append(sum_fitness)\n",
    "        else:\n",
    "            mean_fitness.append(sum_fitness/len(repeats))\n",
    "        \n",
    "    for b in repeats:\n",
    "        axs[case].plot(speeds, results[:,b], color=colors[case], alpha=0.2)    \n",
    "        axs[case].set_title(f\"Case {case+1}\")\n",
    "        axs[case].plot(speeds, mean_fitness, color=colors[case])\n",
    "        axs[3].plot(speeds, mean_fitness, color=colors[case])\n",
    "\n",
    "axs[0].set(xlabel='Speed', ylabel='Fitness')\n",
    "axs[0].set_xticks(np.arange(0.0, 1.1, 0.2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disappearing Fitness vs speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs speed\n",
    "noise = 0\n",
    "speeds = np.linspace(start=0.0, stop=1.0, num=10)\n",
    "visible_periods = [7, 6, 5, 4, 3, 2, 1, 0]\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "results = np.zeros((len(speeds), len(policies)))\n",
    "aucs = np.zeros((len(visible_periods), len(policies)))\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 10), sharex=True, sharey=True)\n",
    "fig.tight_layout(rect=[0,0.1, 1, 0.9])\n",
    "\n",
    "# Test agents\n",
    "for c, period in enumerate(visible_periods):\n",
    "    if period == 6:\n",
    "            period = 10\n",
    "    elif period == 7:\n",
    "        period = 50\n",
    "    for a, speed in enumerate(speeds):\n",
    "        for b, policy in enumerate(policies): \n",
    "            if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "                agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "            elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "                agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "                agnt.alpha = 0.6\n",
    "            elif policy == \"Levy\":\n",
    "                agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "            fitness, _, _, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=case, motion=motion, visible_steps=period, multisensory=multisensory, pc=pc, pm=speed, pe=pe)\n",
    "            results[a, b] = fitness\n",
    "    \n",
    "    idy = 0\n",
    "    idx = c \n",
    "    auc = np.trapz(y=results.T, x=speeds, axis=1)\n",
    "    aucs[c] = auc\n",
    "    \n",
    "    if c >= 4:\n",
    "        idy = 1\n",
    "        idx = c - 4\n",
    "\n",
    "    for b, policy in enumerate(policies): \n",
    "        axs[idy][idx].plot(speeds, results[:,b], color=colors[b])    \n",
    "        axs[idy][idx].set_title(f\"Time to Disappear {period}\")  \n",
    "    \n",
    "axs[0][0].set(xlabel='Speed', ylabel='Fitness')\n",
    "axs[0][0].set_xticks(np.arange(0.0, 1.1, 0.2));\n",
    "fig.subplots_adjust(0, 0.05, 1, 0.95)\n",
    "fig.legend(loc='center right', bbox_to_anchor=(1.15, 0.5), labels=policies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b, policy in enumerate(policies): \n",
    "    plt.plot(visible_periods, aucs[:,b], color=colors[b], label=policy)\n",
    "\n",
    "visible_periods_str = ['0', '1', '2', '3', '4', '5', '10', '50']\n",
    "plt.xticks(range(8), visible_periods_str);\n",
    "plt.xlim(7, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disappearing Fitness vs visible time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs visible time steps\n",
    "noise = 0\n",
    "cases = [\"1\", \"2\", \"3\"]\n",
    "visible_periods = [7, 6, 5, 4, 3, 2, 1, 0]\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "results = np.zeros((len(visible_periods), len(policies)))\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "fig.tight_layout(rect=[0,0.1, 1, 0.9])\n",
    "fig.suptitle(f\"{motion}, {multisensory} - Fitness vs Time to Disappear\")\n",
    "\n",
    "fig2, axs2 = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "fig2.suptitle(f\"{motion}, {multisensory} - Fitness vs Time to Disappear AUC\")\n",
    "\n",
    "# Test agents\n",
    "for c, case in enumerate(cases):\n",
    "    for a, period in enumerate(visible_periods):\n",
    "        if period == 6:\n",
    "            period = 10\n",
    "        elif period == 7:\n",
    "            period = 50\n",
    "        for b, policy in enumerate(policies): \n",
    "            if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "                agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "            elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "                agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "                agnt.alpha = 0.6\n",
    "            elif policy == \"Levy\":\n",
    "                agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "            fitness, _, _, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=case, motion=motion, visible_steps=period, pc=pc, pm=pm, pe=pe)\n",
    "            results[a, b] = fitness\n",
    "    \n",
    "    auc = np.trapz(y=results.T, x=visible_periods, axis=1)\n",
    "\n",
    "    for b, policy in enumerate(policies): \n",
    "        axs[c].plot(visible_periods, results[:,b], color=colors[b])    \n",
    "        axs[c].set_title(f\"Case {case}\")\n",
    "\n",
    "        ml, sl, _ = axs2[c].stem(b, auc[0] - auc[b])\n",
    "        ml.set_color(colors[b])\n",
    "        sl.set_color(colors[b])\n",
    "\n",
    "    axs2[c].set_xticks(range(len(policies)), policies, rotation='vertical')\n",
    "    axs2[c].set_ylabel('AUC');\n",
    "\n",
    "axs[0].set(xlabel='Time to Disappear', ylabel='Fitness', )\n",
    "visible_periods_str = ['0', '1', '2', '3', '4', '5', '10', '50']\n",
    "axs[0].set_xticks(range(8), visible_periods_str);\n",
    "axs[0].xaxis.set_inverted(True)\n",
    "fig.legend(loc='center right', bbox_to_anchor=(1.05, 0.5), labels=policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs visible time steps\n",
    "noise = 0\n",
    "repeats = [i for i in range(10)]\n",
    "cases = [\"1\", \"2\", \"3\"]\n",
    "visible_periods = [7, 6, 5, 4, 3, 2, 1, 0]\n",
    "policy = multimodal_mazes.AgentRuleBasedMemory.policies[1]\n",
    "agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "# policy = multimodal_mazes.AgentRuleBased.policies[7]\n",
    "# agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "colors = [(\"#346ea8\"), (\"#ff760d\"), (\"#fa020f\")]\n",
    "results = np.zeros((len(visible_periods), len(repeats)))\n",
    "\n",
    "# Test agents\n",
    "for a, case in enumerate(cases):\n",
    "    mean_fitness = []\n",
    "    std_fitness = []\n",
    "    for b, period in enumerate(visible_periods):\n",
    "        if period == 6:\n",
    "            period = 10\n",
    "        elif period == 7:\n",
    "            period = 50\n",
    "\n",
    "        fitness_vals = []\n",
    "        \n",
    "        for c in repeats:\n",
    "            fitness, _, _, _ = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=case, motion=motion, visible_steps=period, multisensory=multisensory, pc=pc, pm=pm, pe=pe)\n",
    "            results[b, c] = fitness\n",
    "            fitness_vals.append(fitness)\n",
    "            #print(fitness)\n",
    "        mean_fitness.append(np.mean(fitness_vals))\n",
    "        std_fitness.append(np.std(fitness_vals))\n",
    "    \n",
    "    for c in repeats:\n",
    "        plt.plot(visible_periods, results[:,c], color=colors[a], alpha=0.2)    \n",
    "    \n",
    "    plt.errorbar(visible_periods, mean_fitness, yerr=std_fitness, color=colors[a], label = case)\n",
    "\n",
    "plt.title(f\"{motion}, {multisensory} - {policy} Fitness vs Time to Disappear\")  \n",
    "plt.xlabel(\"Time to Disappear\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "visible_periods_str = ['0', '1', '2', '3', '4', '5', '10', '50']\n",
    "plt.xlim([7, 0])\n",
    "plt.xticks(range(8), visible_periods_str);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Prey Capture Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Prey Capture Order\n",
    "noise = 0\n",
    "n_trials = 50\n",
    "policies = multimodal_mazes.AgentRuleBased.policies + multimodal_mazes.AgentRuleBasedMemory.policies + [\"Levy\"]\n",
    "colors = multimodal_mazes.AgentRuleBased.colors + multimodal_mazes.AgentRuleBasedMemory.colors + [list(np.array([24, 156, 196, 255]) / 255)]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for a, prey in n_prey:\n",
    "\n",
    "    for b, policy in enumerate(policies): \n",
    "        if policy in multimodal_mazes.AgentRuleBased.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBased(location=None, channels=[1,1], policy=policy)\n",
    "        elif policy in multimodal_mazes.AgentRuleBasedMemory.policies:\n",
    "            agnt = multimodal_mazes.AgentRuleBasedMemory(location=None, channels=[1,1], policy=policy)\n",
    "            agnt.alpha = 0.6\n",
    "        elif policy == \"Levy\":\n",
    "            agnt = multimodal_mazes.AgentRandom(location=None, channels=[0,0], motion=policy)\n",
    "\n",
    "        _, times, _, preys = multimodal_mazes.eval_linear_prey_fitness(n_trials=n_trials, height=height, width=width, agnt=agnt, sensor_noise_scale=noise, n_prey=n_prey, pk=pk, n_steps=n_steps, scenario=scenario, case=case, visible_steps=visible_steps, multisensory=multisensory, motion=motion, pc=pc, pm=pm, pe=pe)\n",
    "        #capture_results = {key:[] for key in range(2)}\n",
    "        \n",
    "        for i in range(2):\n",
    "            for trial in range(n_trials):\n",
    "                if preys[trial][i].state == 0:\n",
    "                    #capture_results[i].append(times[trial])\n",
    "            #print(capture_results[i])\n",
    "\n",
    "                    plt.plot(i, times[trial], color=colors[b])\n",
    "\n",
    "plt.ylabel('Capture Time')\n",
    "plt.xlabel('Prey Caught')\n",
    "plt.legend()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multimodal_mazes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
