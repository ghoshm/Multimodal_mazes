{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import neat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "import pickle\n",
    "import multimodal_mazes\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = multimodal_mazes.HMaze(size=11, n_channels=2)\n",
    "maze.generate(number=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "\n",
    "# Colormaps \n",
    "from matplotlib import colors\n",
    "\n",
    "cmap_wall = cm.binary\n",
    "cmap_wall.set_under('k', alpha=0)\n",
    "\n",
    "cmap_ch0 = colors.LinearSegmentedColormap.from_list(\n",
    "    \"\", [\"white\", \"xkcd:ultramarine\"]\n",
    ")\n",
    "\n",
    "cmap_ch1 = colors.LinearSegmentedColormap.from_list(\n",
    "    \"\", [\"white\", \"xkcd:magenta\"]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.imshow(1 - maze.mazes[n][:,:,-1], clim=[0.1,1.0], cmap=cmap_wall, alpha=0.25, zorder=1)\n",
    "\n",
    "# Adjust axes \n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.imshow((cmap_ch0(maze.mazes[n][:,:,0]) + cmap_ch1(maze.mazes[n][:,:,1]))/2, zorder=0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas \n",
    "* Optuna: https://colab.research.google.com/github/araffin/tools-for-robotic-rl-icra2022/blob/main/notebooks/optuna_lab.ipynb#scrollTo=E0yEokTDxhrC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness vs noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs noise\n",
    "noises = np.linspace(start=0.0, stop=0.5, num=21)\n",
    "\n",
    "# Small set\n",
    "wm_flags = np.array([[0,0,0,0,0,0,0], [0,0,0,0,0,0,0], [1,1,1,1,0,0,0],[1,1,1,1,1,1,1]])\n",
    "colors = [\"xkcd:gray\", [0.0, 0.0, 0.0, 0.5], [0.039, 0.73, 0.71, 1], list(np.array([24, 156, 196, 255]) / 255)]\n",
    "\n",
    "# Full set \n",
    "# wm_flags = np.array(list(itertools.product([0,1], repeat=7)))\n",
    "# wm_flags = np.vstack((wm_flags[0], wm_flags))\n",
    "# colors = cm.get_cmap(\"plasma\", len(wm_flags)).colors.tolist()\n",
    "\n",
    "results = np.zeros((len(noises), len(wm_flags)))\n",
    "\n",
    "# Generate mazes\n",
    "maze = multimodal_mazes.TrackMaze(size=11, n_channels=2)\n",
    "maze.generate(number=100000, noise_scale=0.0, gaps=2)\n",
    "\n",
    "maze_test = multimodal_mazes.TrackMaze(size=11, n_channels=2)\n",
    "maze_test.generate(number=1000, noise_scale=0.0, gaps=2)\n",
    "\n",
    "# Run\n",
    "for b, wm_flag in enumerate(tqdm(wm_flags)): \n",
    "\n",
    "    # Control architecture \n",
    "    if b != 1: \n",
    "        n_hidden_units = 8\n",
    "    else:\n",
    "        n_hidden_units = 34 \n",
    "    \n",
    "    # Train\n",
    "    agnt = multimodal_mazes.AgentDQN(location=[5,5], channels=[1,1], sensor_noise_scale=0.05, n_hidden_units=n_hidden_units, wm_flags=wm_flag)\n",
    "    agnt.generate_policy(maze, n_steps=6) \n",
    "\n",
    "    # Test \n",
    "    for a, noise in enumerate(noises):\n",
    "        results[a,b] = multimodal_mazes.eval_fitness(genome=None, config=None, channels=[1,1], sensor_noise_scale=noise, drop_connect_p=0.0, maze=maze_test, n_steps=6, agnt=agnt)\n",
    "\n",
    "# Plotting\n",
    "plt.plot([0.05, 0.05], [0,1], ':', color='k', alpha=0.5, label='Training noise')\n",
    "for b, wm_flag in enumerate(wm_flags): \n",
    "    plt.plot(noises, results[:,b], color=colors[b], label=wm_flag)\n",
    "\n",
    "plt.ylim([0, 1.05])\n",
    "plt.ylabel('Fitness')\n",
    "plt.xlabel('Sensor noise')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs noise AUC \n",
    "auc = np.trapz(y=results.T, x=noises, axis=1)\n",
    "idxs = np.argsort(auc)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5*3,5), sharex=False, sharey=True)\n",
    "for b, idx in enumerate(idxs): \n",
    "    ml, sl, _ = plt.stem(b, auc[idx])\n",
    "    ml.set_color('k')\n",
    "    sl.set_color('k')\n",
    "# plt.xticks(range(len(wm_flags)), policies, rotation='vertical')\n",
    "plt.ylabel('AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_w_norms(agnt):\n",
    "#     \"\"\"\"\"\"\n",
    "#     tmp_f, tmp_o = [], []\n",
    "#     w_norms = np.zeros(9)\n",
    "#     for a, p in enumerate(agnt.parameters()):\n",
    "\n",
    "#         if a <= 1: \n",
    "#             tmp_f.append(torch.norm(p.data, p=\"fro\"))\n",
    "#         else: \n",
    "#             tmp_o.append(torch.norm(p.data, p=\"fro\"))\n",
    "\n",
    "#     w_norms[:2] = np.array(tmp_f) / np.array(tmp_f)[0]\n",
    "#     w_norms[2:][agnt.wm_flags == 1] = np.array(tmp_o) / np.array(tmp_f)[0]\n",
    "\n",
    "#     return w_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "paths = ['../Results/test' + str(n) + '/' for n in range(901, 951)]\n",
    "\n",
    "print(paths)\n",
    "\n",
    "noises = np.linspace(start=0.0, stop=0.5, num=21) # noises,\n",
    "results = np.zeros((len(noises), 129, len(paths))) * np.nan # noises x architectures x repeats\n",
    "wm_flags = np.zeros((129, 7, len(paths))) * np.nan # architectures x flags x repeats\n",
    "n_parameters = np.zeros((129, len(paths))) * np.nan # architectures x repeats\n",
    "auc = np.zeros((129, len(paths))) * np.nan # architectures x repeats \n",
    "\n",
    "training_fs = np.zeros((129, 100, len(paths))) * np.nan  # architectures x data points x repeats\n",
    "input_sens = [[] for _ in range(129)] # architectures \n",
    "input_sens_test_mean = np.zeros((129, len(paths))) * np.nan # architectures x repeats\n",
    "input_sens_overall_mean = np.zeros((129, len(paths))) * np.nan # architectures x repeats\n",
    "memories = np.zeros((129, len(noises), 14, len(paths))) * np.nan # architectures x noises x time steps x repeats\n",
    "# w_norms = np.zeros((129, 9, len(paths))) * np.nan # architectures x weight matricies x repeats\n",
    "\n",
    "gaps = []\n",
    "for a, path in enumerate(tqdm(paths)):\n",
    "\n",
    "    try: \n",
    "        # Load data \n",
    "        for f in os.listdir(path):\n",
    "            if f.endswith(\".pickle\"):\n",
    "                with open(path + f, 'rb') as file:\n",
    "                    agnt = pickle.load(file)\n",
    "                    idx = int(os.path.splitext(f)[0])\n",
    "\n",
    "                    results[:, idx, a] = agnt.results\n",
    "                    wm_flags[idx, :, a] = agnt.wm_flags\n",
    "                    n_parameters[idx, a] = agnt.n_parameters\n",
    "                    auc[idx, a] = np.trapz(y=agnt.results, x=noises)\n",
    "\n",
    "                    training_fs[idx, :, a] = agnt.training_fitness\n",
    "                    input_sens[idx].append(agnt.input_sensitivity)\n",
    "                    input_sens_test_mean[idx, a] = np.nanmean(agnt.input_sensitivity[2][1])\n",
    "                    input_sens_overall_mean[idx, a] = np.nanmean([np.nanmean(agnt.input_sensitivity[i][1]) for i in range(len(agnt.input_sensitivity))])\n",
    "                    memories[idx, :, :, a] = agnt.memory\n",
    "                    # w_norms[idx, :, a] = calculate_w_norms(agnt)\n",
    "\n",
    "            elif f.endswith(\".ini\"):\n",
    "                exp_config = multimodal_mazes.load_exp_config(path + f)\n",
    "                gaps.append(exp_config[\"maze_gaps\"])\n",
    "    except:\n",
    "        gaps.append(np.nan)\n",
    "\n",
    "gaps = np.array(gaps)\n",
    "# assert len(np.unique(gaps)) == 1, \"Loaded experiments with different gap lengths\"\n",
    "print(gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample efficiency \n",
    "\n",
    "# Function as time to threshold\n",
    "se_threshold = np.nanmedian(results[2]) * 0.9\n",
    "sample_efficiency = np.zeros_like(auc) # architectures x repeats\n",
    "\n",
    "for a in range(auc.shape[0]):\n",
    "    for b in range(auc.shape[1]):\n",
    "        if np.isnan(training_fs[a,0,b]) == False:\n",
    "            tmp = np.where(training_fs[a,:,b] >= se_threshold)[0]\n",
    "\n",
    "            if len(tmp) > 1: \n",
    "                sample_efficiency[a,b] = 100 - tmp[0]\n",
    "            else: \n",
    "                sample_efficiency[a,b] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison across hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP Kendall's tau \n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "n_exps = 11\n",
    "n_repeats = 25 \n",
    "\n",
    "exp_keys = np.repeat(np.arange(0, n_exps), repeats=n_repeats)\n",
    "k_taus = np.zeros((n_exps, n_exps)) * np.nan\n",
    "\n",
    "xs, ys = [], []\n",
    "for a in range(n_exps):\n",
    "    for b in range(n_exps):\n",
    "        if a != b: \n",
    "            x = np.zeros(129)\n",
    "            y = np.zeros(129)\n",
    "\n",
    "            x[np.argsort(np.nanmedian(function[:, exp_keys==a], axis=1))] = np.arange(129)\n",
    "            y[np.argsort(np.nanmedian(function[:, exp_keys==b], axis=1))] = np.arange(129)\n",
    "\n",
    "            k_taus[a,b], _ = kendalltau(x, y)\n",
    "\n",
    "            xs.extend(np.copy(x))\n",
    "            ys.extend(np.copy(y))\n",
    "\n",
    "xs = np.array(xs)\n",
    "ys = np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP: Kendall's tau figures \n",
    "interest = []\n",
    "interest.append(np.where(k_taus == np.nanmin(k_taus))[0])\n",
    "interest.append(np.where(k_taus == np.nanmax(k_taus))[0])\n",
    "\n",
    "# Image\n",
    "plt.imshow(k_taus)\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Configuration')\n",
    "plt.colorbar()\n",
    "\n",
    "# Scatters\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5), sharex=False, sharey=False)\n",
    "for a, i in enumerate(interest):\n",
    "    plt.sca(ax[a])\n",
    "    plt.plot([0, 129], [0, 129], 'k', alpha=0.25)\n",
    "\n",
    "    x = np.zeros(129)\n",
    "    y = np.zeros(129)\n",
    "\n",
    "    x[np.argsort(np.nanmedian(function[:, exp_keys==interest[a][0]], axis=1))] = np.arange(129)\n",
    "    y[np.argsort(np.nanmedian(function[:, exp_keys==interest[a][1]], axis=1))] = np.arange(129)\n",
    "    \n",
    "    plt.scatter(x, y, s=3, c='k')\n",
    "\n",
    "    if a == 0:\n",
    "        plt.xlabel('Ranking')\n",
    "        plt.ylabel('Ranking')\n",
    "    else: \n",
    "        plt.xticks([])\n",
    "        plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0, 129], [0, 129], 'k', alpha=1.0, label='y=x')\n",
    "\n",
    "idx = np.argsort(xs)\n",
    "curve = np.poly1d(np.polyfit(xs[idx],ys[idx],deg=1))\n",
    "plt.plot(xs[idx], curve(xs[idx]), color='xkcd:turquoise', alpha=1.0, label='Linear fit')\n",
    "\n",
    "plt.scatter(xs, ys, s=1, c='k', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing hyperparameters \n",
    "\n",
    "exp_keys = np.repeat(np.arange(0,n_exps), repeats=n_repeats)\n",
    "np.argsort(np.nanmedian(function, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest = [128, 41]\n",
    "\n",
    "i_cols = ['xkcd:purple', 'xkcd:dark seafoam']\n",
    "i_labels = ['Fully recurrent', 'Fastest learner']\n",
    "\n",
    "tmp = []\n",
    "for a, i in enumerate(interest):\n",
    "    tmp.append([np.nanmedian(function[i, exp_keys==e]) for e in np.unique(exp_keys)])\n",
    "\n",
    "tmp = np.array(tmp)\n",
    "plt.plot([range(n_exps), range(n_exps)], [tmp[0], tmp[1]], c='xkcd:grey', zorder=0);\n",
    "\n",
    "for a, i in enumerate(interest):\n",
    "    plt.scatter(range(n_exps), tmp[a], c=i_cols[a], label=i_labels[a])\n",
    "\n",
    "plt.ylabel('Fitness')\n",
    "plt.xticks(range(11), ['Baseline', 'n=4', 'n=16', 'sn=0.025', 'sn=0.1', 'lr=0.0001', 'lr=0.01', 'g=0.95', 'g=0.99', 'eps=50000', 'eps=200000'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness and robustness to noise (Averaging per experiment)\n",
    "interest = [1,128, np.argmax(np.nanmedian(function, axis=1)), np.argmax(np.nanmedian(auc, axis=1))]\n",
    "i_cols = ['xkcd:dusty blue', 'xkcd:purple', 'xkcd:dark seafoam', 'xkcd:yellowish']\n",
    "i_labels = ['Feedforward (L)', 'Fully recurrent', 'Fastest learner', 'Most robust']\n",
    "\n",
    "tmp_f = np.array([np.nanmedian(function[:, exp_keys == e], axis=1) for e in range(11)]).T\n",
    "multimodal_mazes.plot_dqn_rankings(y=tmp_f, y_label='Fitness over training', interest=interest, i_cols=i_cols, sig_test=interest[1])\n",
    "\n",
    "tmp_r = np.array([np.nanmedian(auc[:, exp_keys == e], axis=1) for e in range(11)]).T\n",
    "multimodal_mazes.plot_dqn_rankings(y=tmp_r, y_label='Robustness', interest=interest, i_cols=i_cols, sig_test=interest[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness, sample efficiency and robustness to noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks of interest\n",
    "interest = [1,128, np.argmax(np.nanmedian(sample_efficiency, axis=1)), np.argmax(np.nanmedian(auc, axis=1))]\n",
    "i_cols = ['xkcd:soft blue', 'xkcd:purpley', 'xkcd:magenta', 'xkcd:bright orange']\n",
    "i_labels = ['Feedforward (L)', 'Fully connected', 'Most sample efficient', 'Most robust']\n",
    "\n",
    "# np.argwhere((np.nanmax(wm_flags, axis=2) == [1,0,0,1,0,0,0]).all(1))\n",
    "\n",
    "print(interest)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(i_cols), figsize=(5*len(i_cols),5), sharex=True, sharey=True)\n",
    "for a, i in enumerate(interest):\n",
    "    plt.sca(ax[a])\n",
    "    multimodal_mazes.plot_dqn_architecture(np.nanmax(wm_flags, axis=2)[interest[a]], ax=ax[a], color=i_cols[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select architectures \n",
    "multimodal_mazes.plot_dqn_examples(y=results[2], y_label='Fitness', interest=interest, i_cols=i_cols, sig_test=interest[1])\n",
    "multimodal_mazes.plot_dqn_examples(y=sample_efficiency, y_label='Sample efficiency', interest=interest, i_cols=i_cols, sig_test=interest[1])\n",
    "multimodal_mazes.plot_dqn_examples(y=auc, y_label='Robustness', interest=interest, i_cols=i_cols, sig_test=interest[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All architectures \n",
    "multimodal_mazes.plot_dqn_rankings(y=results[2], y_label='Fitness', interest=interest, i_cols=i_cols, sig_test=interest[1])\n",
    "multimodal_mazes.plot_dqn_rankings(y=sample_efficiency, y_label='Sample efficiency', interest=interest, i_cols=i_cols, sig_test=interest[1])\n",
    "multimodal_mazes.plot_dqn_rankings(y=auc, y_label='Robustness', interest=interest, i_cols=i_cols, sig_test=interest[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters vs function (average)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10*3,5), sharex=True, sharey=False)\n",
    "\n",
    "for a in range(3):\n",
    "    plt.sca(ax[a])\n",
    "\n",
    "    x = np.nanmean(n_parameters, axis=1)\n",
    "    plt.xlabel('Number of parameters')\n",
    "\n",
    "    if a == 0: \n",
    "        y = np.nanmedian(results[2], axis=1)\n",
    "        plt.ylabel('Fitness')\n",
    "\n",
    "    elif a == 1:\n",
    "        y = np.nanmedian(sample_efficiency, axis=1)\n",
    "        plt.ylabel('Sample efficiency')\n",
    "\n",
    "    elif a == 2:\n",
    "        y = np.nanmedian(auc, axis=1)\n",
    "        plt.ylabel('Robustness')\n",
    "\n",
    "    plt.scatter(x, y, s=90, c='k', alpha=0.25)\n",
    "\n",
    "    for b, i in enumerate(interest):\n",
    "        plt.scatter(x[i], y[i], color=i_cols[b], s=90, alpha=1.0, label=i_labels[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness vs noise \n",
    "plt.plot([0.05, 0.05], [0,1], ':', color='k', alpha=0.5, label='Training noise')\n",
    "\n",
    "plt.plot([], [], 'k', alpha=0.1, label='All architectures')\n",
    "for a, i in enumerate(range(129)):\n",
    "    plt.plot(noises, np.nanmedian(results[:,a], axis=1), color='k', alpha=0.1)\n",
    "\n",
    "for a, i in enumerate(interest):\n",
    "    plt.plot(noises, np.nanmedian(results[:,i], axis=1), color=i_cols[a], label=i_labels[a])\n",
    "\n",
    "plt.ylim([0, 1.05])\n",
    "plt.ylabel('Fitness')\n",
    "plt.xlabel('Sensor noise')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample efficiency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.argsort(np.nanmedian(sample_efficiency, axis=1)))\n",
    "\n",
    "# interest = [87, 128, np.argmax(np.nanmedian(sample_efficiency, axis=1))]\n",
    "\n",
    "# i_cols = ['k', 'xkcd:purple', 'xkcd:dark seafoam green']\n",
    "# i_labels = ['Min', 'Fully recurrent', 'Max']\n",
    "\n",
    "# print(interest)\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=len(interest), figsize=(5*len(interest),5), sharex=True, sharey=True)\n",
    "# for a, i in enumerate(interest):\n",
    "#     plt.sca(ax[a])\n",
    "#     multimodal_mazes.plot_dqn_architecture(np.nanmax(wm_flags, axis=2)[interest[a]], ax=ax[a], color=i_cols[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot median training curve + interquartile range\n",
    "# Captures the middle 50% of the data\n",
    "\n",
    "# All networks\n",
    "x = range(training_fs.shape[1])\n",
    "l, m, u = np.nanquantile(np.transpose(training_fs, (1,0,2)).reshape(training_fs.shape[1], -1), [0.25, 0.5, 0.75], axis=1)\n",
    "plt.plot(x, m, color='xkcd:grey', label='All networks')\n",
    "plt.fill_between(x, l, u, color='xkcd:grey', alpha=0.25, edgecolor=None)\n",
    "\n",
    "# Interest\n",
    "for a, i in enumerate(interest):\n",
    "    x = range(training_fs.shape[1])\n",
    "    l, m, u = np.nanquantile(training_fs[i], [0.25, 0.5, 0.75], axis=1)\n",
    "    plt.plot(x, m, color=i_cols[a], label=i_labels[a])\n",
    "    plt.fill_between(x, l, u, color=i_cols[a], alpha=0.25, edgecolor=None)\n",
    "\n",
    "plt.xticks([0, 50, 100], labels=[\"0\", \"100000\", \"200000\"])\n",
    "plt.xlabel('Training episodes')\n",
    "# plt.ylim([0, 1.05])\n",
    "plt.ylabel('Fitness')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input sensitivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argsort(np.nanmedian(input_sens_test_mean, axis=1))\n",
    "# interest = [np.argmin(np.nanmedian(input_sens_test_mean, axis=1)), 128, 17]\n",
    "\n",
    "# i_cols = ['k', 'xkcd:purple', 'xkcd:dark seafoam green']\n",
    "# i_labels = ['Min', 'Fully recurrent', 'Max']\n",
    "# print(interest)\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=len(interest), figsize=(5*len(interest),5), sharex=True, sharey=True)\n",
    "# for a, i in enumerate(interest):\n",
    "#     plt.sca(ax[a])\n",
    "#     multimodal_mazes.plot_dqn_architecture(np.nanmax(wm_flags, axis=2)[interest[a]], ax=ax[a], color=i_cols[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sens: lists of architectures, networks, noise levels then x and y\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def gaussian(x, amp, mean, stddev):\n",
    "    return amp * np.exp(-((x - mean) ** 2) / (2 * stddev ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit curves (per network)\n",
    "# Input sens: lists of architectures, networks, noise levels then x and y\n",
    "\n",
    "for a in interest:\n",
    "    for b in range(len(input_sens[a])):\n",
    "\n",
    "        xs = input_sens[a][b][2][0]\n",
    "        ys = input_sens[a][b][2][1]\n",
    "        idx = np.argsort(xs)\n",
    "\n",
    "        try:\n",
    "            popt, _ = curve_fit(gaussian, xs[idx], ys[idx], bounds=((-np.inf, -np.inf, 0), (np.inf, np.inf, np.inf)))\n",
    "            y_fit = gaussian(xs[idx], *popt)\n",
    "            plt.plot(xs[idx], y_fit, color=i_cols[np.where(np.array(interest) == a)[0][0]], alpha=0.75)\n",
    "        except:\n",
    "            pass \n",
    "\n",
    "plt.xticks(ticks=[-1, 0, 1], labels=(\"-1.0\", \"0.0\", \"1.0\"))\n",
    "plt.xlabel('Input state (L:R)')\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "\n",
    "for a, i in enumerate(interest):\n",
    "    plt.plot([], [], i_cols[a], label=i_labels[a])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit curves (per architecture)\n",
    "\n",
    "for a, i in enumerate(interest):\n",
    "\n",
    "    xs, ys = [], []\n",
    "\n",
    "    for b in range(len(input_sens[i])):\n",
    "\n",
    "        xs.extend(input_sens[i][b][2][0])\n",
    "        ys.extend(input_sens[i][b][2][1])\n",
    "    \n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    idx = np.argsort(xs)\n",
    "\n",
    "    try:\n",
    "        popt, _ = curve_fit(gaussian, xs[idx], ys[idx], bounds=((-np.inf, -np.inf, 0), (np.inf, np.inf, np.inf)))\n",
    "        y_fit = gaussian(xs[idx], *popt)\n",
    "        plt.plot(xs[idx], y_fit, color=i_cols[a], alpha=1.0)\n",
    "    except:\n",
    "        pass \n",
    "\n",
    "plt.xticks(ticks=[-1, 0, 1], labels=(\"-1.0\", \"0.0\", \"1.0\"))\n",
    "plt.xlabel('Input state (L:R)')\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "\n",
    "for a, i in enumerate(interest):\n",
    "    plt.plot([], [], i_cols[a], label=i_labels[a])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All architectures \n",
    "multimodal_mazes.plot_dqn_rankings(y=input_sens_test_mean, y_label=\"Input sensitivity\", interest=interest, i_cols=i_cols, sig_test=interest[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memories: architectures x noises x time steps x repeats\n",
    "\n",
    "memory_capacity_test = np.nansum(memories[:,2], axis=1) # architectures x repeats\n",
    "\n",
    "# np.argsort(np.nanmedian(memory_capacity_test, axis=1))\n",
    "# interest = [1, 128, np.argmax(np.nanmedian(memory_capacity_test, axis=1))]\n",
    "\n",
    "# i_cols = ['k', 'xkcd:purple', 'xkcd:dark seafoam green']\n",
    "# i_labels = ['Feedforward (L)', 'Fully recurrent', 'Max']\n",
    "# print(interest)\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=len(interest), figsize=(5*len(interest),5), sharex=True, sharey=True)\n",
    "# for a, i in enumerate(interest):\n",
    "#     plt.sca(ax[a])\n",
    "#     multimodal_mazes.plot_dqn_architecture(np.nanmax(wm_flags, axis=2)[interest[a]], ax=ax[a], color=i_cols[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "\n",
    "# All networks\n",
    "x = range(memories.shape[2])\n",
    "l, m, u = np.nanquantile(np.transpose(memories[:,2], (1,0,2)).reshape(memories.shape[2],-1), [0.25, 0.5, 0.75], axis=1)\n",
    "plt.plot(x, m[::-1], color='xkcd:grey', label=\"All networks\")\n",
    "plt.fill_between(x, l[::-1], u[::-1], color='xkcd:grey', alpha=0.25, edgecolor=None)\n",
    "\n",
    "# Interest\n",
    "for a, i in enumerate(interest):\n",
    "    x = range(memories[i,2].shape[0])\n",
    "    l, m, u = np.nanquantile(memories[i,2], [0.25, 0.5, 0.75], axis=1)\n",
    "    plt.plot(x, m[::-1], color=i_cols[a], label=i_labels[a])\n",
    "    plt.fill_between(x, l[::-1], u[::-1], color=i_cols[a], alpha=0.25, edgecolor=None)\n",
    "\n",
    "plt.hlines(y=1.0, xmin=0, xmax=5, color='k', linestyles='--')\n",
    "plt.ylim(-0.95, 5)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Input-output influence')\n",
    "plt.xticks(np.arange(memories.shape[2]), labels=np.arange(memories.shape[2])[::-1]*-1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_mazes.plot_dqn_rankings(y=memory_capacity_test, y_label=\"Memory capacity\", interest=interest, i_cols=i_cols, sig_test=interest[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capacities vs parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters vs function (average)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10*2,5), sharex=True, sharey=False)\n",
    "\n",
    "for a in range(2):\n",
    "    plt.sca(ax[a])\n",
    "\n",
    "    x = np.nanmean(n_parameters, axis=1)\n",
    "    plt.xlabel('Number of parameters')\n",
    "\n",
    "    if a == 0: \n",
    "        y = np.nanmedian(input_sens_test_mean, axis=1)\n",
    "        plt.ylabel('Input sensitivity')\n",
    "\n",
    "    elif a == 1:\n",
    "        y = np.nanmedian(memory_capacity_test, axis=1)\n",
    "        plt.ylabel('Memory')\n",
    "\n",
    "    plt.scatter(x, y, s=90, c='k', alpha=0.25)\n",
    "\n",
    "    for b, i in enumerate(interest):\n",
    "        plt.scatter(x[i], y[i], color=i_cols[b], s=90, alpha=1.0, label=i_labels[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison across tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_removal_settings(idx_a, idx_b, wm_flags):\n",
    "\n",
    "    wm_flags = np.nanmax(wm_flags, axis=2)\n",
    "    A = wm_flags[idx_a].astype(int)\n",
    "    B = wm_flags[idx_b].astype(int)\n",
    "\n",
    "    # Ensure A has more or equal 1s compared to B\n",
    "    if np.sum(A) < np.sum(B):\n",
    "        raise ValueError(\"A should have more parameters than B\")\n",
    "\n",
    "    # Generate all possible settings by removing elements from A\n",
    "    removal_settings = []\n",
    "    for i in range(1, wm_flags.shape[1] + 1):\n",
    "        for combination in itertools.combinations(np.where(A == 1)[0], i):\n",
    "            new_setting = A.copy()\n",
    "            new_setting[list(combination)] = 0\n",
    "            if np.array_equal(new_setting & B, B):\n",
    "                removal_settings.append(new_setting)\n",
    "\n",
    "    removal_settings = np.vstack((A, np.array(removal_settings)))\n",
    "    removal_settings = removal_settings[np.argsort(np.sum(removal_settings, axis=1))]\n",
    "\n",
    "    idxs = []\n",
    "    for a in removal_settings:\n",
    "        idxs.extend(np.argwhere((wm_flags == list(a)).all(1)))\n",
    "\n",
    "    return list(np.array(idxs).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks of interest\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# np.argwhere((np.nanmax(wm_flags, axis=2) == [1,0,0,1,0,0,0]).all(1)\n",
    "\n",
    "interest = find_removal_settings(idx_a=128, idx_b=np.argmax(np.nanmedian(results[2,:,:], axis=1)), wm_flags=wm_flags)[::-1]\n",
    "\n",
    "i_cols = LinearSegmentedColormap.from_list(\n",
    "     \"\", ['xkcd:purple', 'xkcd:dark seafoam'], N=len(interest)\n",
    ")\n",
    "i_labels = ['Fully recurrent', \"Most accurate\"]\n",
    "for _ in range(len(interest) - 2):\n",
    "     i_labels.insert(1, \"\")\n",
    "print(interest)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(interest), figsize=(len(interest)*5,5), sharex=True, sharey=True)\n",
    "\n",
    "for a, i in enumerate(interest):\n",
    "     plt.sca(ax[a])\n",
    "     multimodal_mazes.plot_dqn_architecture(np.nanmax(wm_flags, axis=2)[interest[a]], ax=ax[a], color=i_cols(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness plot\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "offsets = np.linspace(start=-0.35, stop=0.35, num=len(interest))\n",
    "\n",
    "for a, i in enumerate(interest):\n",
    "    for b in range(3):\n",
    "        l, m, u = np.nanquantile(results[2][i][gaps==b], [0.25, 0.5, 0.75])\n",
    "        plt.scatter(offsets[a] + b, m, color=i_cols(a))\n",
    "        plt.plot([offsets[a] + b, offsets[a] + b], [l, u], color=i_cols(a), alpha=0.5)\n",
    "\n",
    "plt.xticks([0, 1, 2])\n",
    "plt.xlabel('Gap')\n",
    "plt.ylabel('Fitness')\n",
    "\n",
    "for a, i in enumerate(interest):\n",
    "    plt.scatter([], [], color=i_cols(a), label=i_labels[a])\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_capacity_test = np.sum(memories[:,2], axis=1) # architectures x repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "gap = 2\n",
    "\n",
    "_, p = stats.mannwhitneyu(\n",
    "    x=training_fs_n_auc[interest[0]][gaps == gap], y=training_fs_n_auc[interest[-1]][gaps == gap], method=\"asymptotic\", nan_policy=\"omit\"\n",
    ")\n",
    "print(p)\n",
    "\n",
    "_, p = stats.mannwhitneyu(\n",
    "    x=input_sens_test_mean[interest[0]][gaps == gap], y=input_sens_test_mean[interest[-1]][gaps == gap], method=\"asymptotic\", nan_policy=\"omit\"\n",
    ")\n",
    "print(p)\n",
    "\n",
    "_, p = stats.mannwhitneyu(\n",
    "    x=memory_capacity_test[interest[0]][gaps == gap], y=memory_capacity_test[interest[-1]][gaps == gap], method=\"asymptotic\", nan_policy=\"omit\"\n",
    ")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, i in enumerate(interest):\n",
    "    for b in range(3):\n",
    "        # l, m, u = np.nanquantile(input_sens_test_mean[i][gaps==b], [0.25, 0.5, 0.75])\n",
    "        # l, m, u = np.nanquantile(memory_capacity_test[i][gaps==b] - np.nanmedian(memory_capacity_test[0][gaps == b]), [0.25, 0.5, 0.75])\n",
    "        \n",
    "        l, m, u = np.nanquantile(memory_capacity_test[i][gaps==b], [0.25, 0.5, 0.75])\n",
    "        plt.scatter(offsets[a] + b, m, color=i_cols(a))\n",
    "        plt.plot([offsets[a] + b, offsets[a] + b], [l, u], color=i_cols(a), alpha=0.5)\n",
    "\n",
    "plt.xlabel('Gap')\n",
    "plt.ylabel('Memory')\n",
    "\n",
    "for a, i in enumerate(interest):\n",
    "    plt.scatter([], [], color=i_cols(a), label=i_labels[a])\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph features \n",
    "import networkx as nx\n",
    "\n",
    "Gs = []\n",
    "for a, wm_flag in enumerate(np.nanmax(wm_flags, axis=2)):\n",
    "    G = nx.DiGraph([(0,1), (1,2)]) # F0 and F1 \n",
    "    if wm_flag[0]: G.add_edge(0,0) # L0\n",
    "    if wm_flag[1]: G.add_edge(1,1) # L1\n",
    "    if wm_flag[2]: G.add_edge(2,2) # L2\n",
    "    if wm_flag[3]: G.add_edge(0,2) # S0\n",
    "    if wm_flag[4]: G.add_edge(2,0) # S1\n",
    "    if wm_flag[5]: G.add_edge(1,0) # B0\n",
    "    if wm_flag[6]: G.add_edge(2,1) # B1\n",
    "\n",
    "    Gs.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Nodes and edges \n",
    "wm_flags = np.array(list(itertools.product([0, 1], repeat=7)))\n",
    "fig, ax = plt.subplots(nrows=8, ncols=16, figsize=(30,15), sharex=True, sharey=True)\n",
    "\n",
    "for a, _ in enumerate(ax.ravel()):\n",
    "    plt.sca(ax.ravel()[a])\n",
    "\n",
    "    multimodal_mazes.plot_dqn_architecture(wm_flags[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single unit-unit matrices\n",
    "multimodal_mazes.plot_dqn_weight_matrix(n_input_units=8, n_hidden_units=8, n_output_units=5, wm_flag=np.ones(7))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.xlabel('Units')\n",
    "plt.ylabel('Units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All unit-unit matrices \n",
    "wm_flags = np.array(list(itertools.product([0, 1], repeat=7)))\n",
    "fig, ax = plt.subplots(nrows=8, ncols=16, figsize=(30,15), sharex=True, sharey=True)\n",
    "\n",
    "for a, _ in enumerate(ax.ravel()):\n",
    "    plt.sca(ax.ravel()[a])\n",
    "\n",
    "    multimodal_mazes.plot_dqn_weight_matrix(n_input_units=8, n_hidden_units=8, n_output_units=5, wm_flag=wm_flags[a])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Shapley Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = np.nanmean(results[2,:,:],1) # test accuracy \n",
    "scores = np.nanmean(auc, axis=1) # robustness to noise\n",
    "wm_flags = np.nanmax(wm_flags, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def calculate_shapley_values(scores, wm_flags):\n",
    "    n_hyperparams = wm_flags.shape[1]\n",
    "    shapley_values = np.zeros(n_hyperparams)\n",
    "\n",
    "    # Iterate over each hyperparameter\n",
    "    for i in range(n_hyperparams):\n",
    "        marginal_contributions = []\n",
    "        \n",
    "        # Iterate over all possible coalitions\n",
    "        for coalition_size in range(n_hyperparams):\n",
    "            for coalition in combinations(range(n_hyperparams), coalition_size):\n",
    "                if i not in coalition:\n",
    "                    # Convert coalition to list and add the current hyperparameter\n",
    "                    coalition_with_i = list(coalition) + [i]\n",
    "                    \n",
    "                    # Calculate the indices for the coalitions with and without the hyperparameter\n",
    "                    idx_coalition = np.where(np.all(wm_flags[:, coalition] == 1, axis=1) & (wm_flags[:,i] == 0))[0]\n",
    "                    idx_coalition_with_i = np.where(np.all(wm_flags[:, coalition_with_i] == 1, axis=1))[0]\n",
    "                    assert len(np.intersect1d(idx_coalition, idx_coalition_with_i)) == 0, \"Indexing error\"\n",
    "                    \n",
    "                    # Calculate the marginal contribution and add it to the list\n",
    "                    marginal_contributions.extend(scores[idx_coalition_with_i] - scores[idx_coalition])\n",
    "        \n",
    "        # Calculate the Shapley value for the current hyperparameter as the average of its marginal contributions\n",
    "        shapley_values[i] = np.mean(marginal_contributions)\n",
    "        \n",
    "        return shapley_values\n",
    "\n",
    "# Calculate \n",
    "shapley_values = calculate_shapley_values(scores, wm_flags)\n",
    "plt.scatter(range(len(shapley_values)), shapley_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capacities vs function (average)\n",
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(5*2,5*3), sharex=\"col\", sharey=\"row\")\n",
    "\n",
    "for a in range(2):\n",
    "    for b in range(3):\n",
    "        plt.sca(ax[b,a])\n",
    "\n",
    "        # Data \n",
    "        if a == 0: \n",
    "            x = np.nanmedian(input_sens_test_mean, axis=1)\n",
    "            plt.xlabel('Input sensitivity')\n",
    "\n",
    "        elif a == 1:\n",
    "            x = np.nanmedian(memory_capacity_test, axis=1)\n",
    "            plt.xlabel('Memory')\n",
    "\n",
    "        if b == 0: \n",
    "            y = np.nanmedian(results[2], axis=1)\n",
    "            plt.ylabel('Fitness')\n",
    "\n",
    "        elif b == 1:\n",
    "            y = np.nanmedian(sample_efficiency, axis=1)\n",
    "            plt.ylabel('Sample efficiency')\n",
    "\n",
    "        elif b == 2:\n",
    "            y = np.nanmedian(auc, axis=1)\n",
    "            plt.ylabel('Robustness')\n",
    "\n",
    "        # Plot \n",
    "        x = x[np.nanmedian(results[2], axis=1) >= 0.75]\n",
    "        y = y[np.nanmedian(results[2], axis=1) >= 0.75]\n",
    "\n",
    "        idx = np.argsort(x)\n",
    "        curve = np.poly1d(np.polyfit(x[idx],y[idx],deg=2))\n",
    "        ax[b,a].plot(x[idx], curve(x[idx]), color='k', alpha=1.0)\n",
    "\n",
    "        plt.scatter(x, y, s=90, c='k', alpha=0.25)\n",
    "        if a != 0:     \n",
    "            ax[b,a].get_yaxis().set_visible(False)\n",
    "        if b != 2: \n",
    "            ax[b,a].get_xaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capacities vs function (average)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(5*3,5), sharex=\"all\", sharey=\"all\")\n",
    "\n",
    "cb_labels = ['Fitness', 'Sample efficiency', 'Robustness']\n",
    "\n",
    "def min_max_norm(x): \n",
    "    return (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "for a in range(3):\n",
    "    plt.sca(ax[a])\n",
    "\n",
    "    if a == 0: \n",
    "        c = np.nanmedian(results[2], axis=1) # fitness\n",
    "    elif a == 1: \n",
    "        c = np.nanmedian(sample_efficiency, axis=1) # sample efficiency\n",
    "    elif a == 2: \n",
    "        c = np.nanmedian(auc, axis=1) # robustness\n",
    "\n",
    "    x = np.nanmedian(input_sens_test_mean, axis=1)\n",
    "    y = np.nanmedian(memory_capacity_test, axis=1)\n",
    "\n",
    "    x = x[np.where(np.nanmedian(results[2], axis=1) >= 0.6)[0]]\n",
    "    y = y[np.where(np.nanmedian(results[2], axis=1) >= 0.6)[0]]\n",
    "    c = c[np.where(np.nanmedian(results[2], axis=1) >= 0.6)[0]]\n",
    "\n",
    "    plt.scatter(min_max_norm(x), min_max_norm(y), c=c, s=90, alpha=0.75)\n",
    "\n",
    "    plt.xlabel('Input sensitivity')\n",
    "    plt.ylabel('Memory')\n",
    "    plt.colorbar(location=\"top\", shrink=0.75, label=cb_labels[a])\n",
    "\n",
    "    if a != 0:     \n",
    "        ax[a].get_yaxis().set_visible(False)\n",
    "        ax[a].get_xaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fits\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "X = np.hstack((np.transpose(wm_flags, (0, 2, 1)).reshape(-1,7), # structure\n",
    "               input_sens_test_mean.reshape(-1)[:,None], # sensitivity \n",
    "               memory_capacity_test.reshape(-1)[:, None])) # memory \n",
    "\n",
    "y = np.hstack((input_sens_test_mean.reshape(-1)[:,None], # sensitivity\n",
    "                memory_capacity_test.reshape(-1)[:, None], # memory\n",
    "                results[2].reshape(-1)[:, None], # fitness\n",
    "                sample_efficiency.reshape(-1)[:, None], # sample efficiency \n",
    "                auc.reshape(-1)[:, None])) # robustness\n",
    "\n",
    "x_masks = np.zeros((13, X.shape[1])) # no features\n",
    "for i in range(9):\n",
    "    x_masks[i+1, i] = 1\n",
    "x_masks[10, 7:] = 1 # capacity \n",
    "x_masks[11, :7] = 1 # structure\n",
    "x_masks[12] = 1 # all features \n",
    "\n",
    "cv_means = np.zeros((x_masks.shape[0], y.shape[1])) * np.nan # masks x predictions\n",
    "cv_stds = np.zeros_like(cv_means) * np.nan # masks x predictions\n",
    "\n",
    "for a in tqdm(range(x_masks.shape[0])):\n",
    "\n",
    "    for b in range(y.shape[1]):\n",
    "        \n",
    "        if a == 0: \n",
    "            cv_means[a,b] = np.sqrt(np.mean((y[:,b] - np.mean(y[:,b])) ** 2))\n",
    "\n",
    "        else: \n",
    "            RF_model = RandomForestRegressor()\n",
    "            kf = KFold(n_splits=10, shuffle=True)\n",
    "            scores = cross_val_score(RF_model, X[:, np.where(x_masks[a])[0]], y[:,b], cv=kf, scoring=\"neg_root_mean_squared_error\") * -1\n",
    "            \n",
    "            cv_means[a,b] = np.mean(scores)\n",
    "            cv_stds[a,b] = np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errorbar plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(5*5,5), sharex=\"none\", sharey=\"all\")\n",
    "x_labels = ['Sensitivity (RMSE)', 'Memory (RMSE)', 'Fitness (RMSE)', 'Sample efficiency (RMSE)', 'Robustness (RMSE)']\n",
    "y_labels = [\"No features\", \"$W_{ii}$\", \"$W_{hh}$\", \"$W_{oo}$\", \"$W_{io}$\", \"$W_{oi}$\", \"$W_{hi}$\", \"$W_{oh}$\", \"Input sensitivity\", \"Memory\", \"Capacity\", \"Structure\", \"All features\"]\n",
    "\n",
    "for a in range(len(x_labels)):\n",
    "    plt.sca(ax[a])\n",
    "\n",
    "    plt.errorbar(x=cv_means[:,a], y=range(x_masks.shape[0]), xerr=cv_stds[:,a], fmt='.', c='xkcd:grey')\n",
    "\n",
    "    plt.xlabel(x_labels[a])\n",
    "    \n",
    "    if a == 0: \n",
    "        plt.yticks(range(len(y_labels)), y_labels)\n",
    "    else:  \n",
    "        ax[a].get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure-capacity-function plot \n",
    "y_pred = 4\n",
    "cv_means_norm = (cv_means[0] - cv_means) / cv_means[0] # models x predictions\n",
    "\n",
    "import matplotlib\n",
    "cmap = plt.cm.get_cmap('PiYG')\n",
    "norm = matplotlib.colors.Normalize(vmin=-1, vmax=1)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Nodes \n",
    "plt.scatter(x=np.zeros(7), y=range(7), c='xkcd:light grey', zorder=1)\n",
    "plt.scatter(x=np.ones(2), y=[2.5, 3.5],c='xkcd:light grey', zorder=1)\n",
    "plt.scatter(x=2, y=3, c='xkcd:light grey', zorder=1)\n",
    "\n",
    "# Edges \n",
    "for i in range(7):\n",
    "\n",
    "    # X -> C\n",
    "    for a, o in enumerate([2.5, 3.5]):\n",
    "        try: \n",
    "            plt.plot([0,1], [i,o], c=cmap(norm(cv_means_norm[i+1,a])), zorder=0)\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "    # X -> Y \n",
    "    plt.plot([0,1], [i, i], c=cmap(norm(cv_means_norm[i+1,y_pred])), zorder=0)\n",
    "    plt.plot([1,2], [i, 3], c=cmap(norm(cv_means_norm[i+1,y_pred])), zorder=0)\n",
    "\n",
    "# C -> Y \n",
    "for a, i in enumerate([2.5, 3.5]):\n",
    "    plt.plot([1, 2], [i,3], c=cmap(norm(cv_means_norm[a+8,y_pred])), zorder=0)\n",
    "\n",
    "plt.yticks(range(7), labels=[\"$W_{ii}$\", \"$W_{hh}$\", \"$W_{oo}$\", \"$W_{io}$\", \"$W_{oi}$\", \"$W_{hi}$\", \"$W_{oh}$\"])\n",
    "\n",
    "plt.xticks([0,1,2], labels=[\"\", 'Ic, Mc', x_labels[y_pred]], rotation=90)\n",
    "\n",
    "ax.spines[['left', 'bottom']].set_visible(False)\n",
    "ax.tick_params(left=False, bottom=False)\n",
    "\n",
    "cb = fig.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "             ax=ax, orientation='vertical', label='Meep', shrink=0.5)\n",
    "cb.outline.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import shap\n",
    "import seaborn as sns\n",
    "\n",
    "# Structure\n",
    "X = np.transpose(wm_flags, (0, 2, 1)).reshape(-1,7)\n",
    "\n",
    "# Capacity (for test fitness)\n",
    "# X = np.zeros((len(auc.reshape(-1)), 2)) * np.nan\n",
    "# X[:,0] = input_sens_test_mean.reshape(-1) # input sensitivity \n",
    "# X[:,1] = memory_capacity_test.reshape(-1) # memory capacity \n",
    "\n",
    "# Capacity (for robustness to noise)\n",
    "# X = np.zeros((len(training_fs_n_auc.reshape(-1)), 3)) * np.nan\n",
    "# X[:,1] = input_sens_overall_mean.reshape(-1) # input sensitivity \n",
    "# X[:,2] = np.nanmean(np.nansum(memories[:,:,1:], axis=2), axis=1).reshape(-1) # memory capacity \n",
    "\n",
    "# Function \n",
    "# y = input_sens_test_mean.reshape(-1) # input sensitivity\n",
    "y = memory_capacity_test.reshape(-1) # memory capacity\n",
    "\n",
    "# y = results[2].reshape(-1) # fitness\n",
    "# y = sample_efficiency.reshape(-1) # sample efficiency\n",
    "# y = auc.reshape(-1) # robustness to noise \n",
    "\n",
    "# Remove nan values \n",
    "X = X[np.isnan(y) == False]\n",
    "y = y[np.isnan(y) == False]\n",
    "assert len(X) == len(y), \"Mismatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit random forest model\n",
    "RF_model = RandomForestRegressor()\n",
    "# kf = KFold(n_splits=10, shuffle=True)\n",
    "# scores = cross_val_score(RF_model, X, y, cv=kf, scoring=\"neg_mean_absolute_error\") * -1\n",
    "# print(\"Mean guess \" + str(np.mean(abs(np.mean(y) - y))) + \" +/- \" + str(np.std(abs(np.mean(y) - y))))\n",
    "# print(str(np.round(np.mean(scores),3)) + \" +/- \" + str(np.round(np.std(scores),3)))\n",
    "RF_model.fit(X,y)\n",
    "print(np.mean(abs(RF_model.predict(X) - y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fit \n",
    "plt.scatter(RF_model.predict(X), y, c='k',marker='.', linewidths=0, alpha=0.1, label='All networks')\n",
    "plt.plot([0,1], [0,1], c='xkcd:turquoise', label='y=x')\n",
    "\n",
    "# plt.xlim([0, 0.55])\n",
    "# plt.ylim([0, 0.55])\n",
    "plt.xlabel('Predicted robustness')\n",
    "plt.ylabel('Robustness')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shap analysis\n",
    "explainer = shap.TreeExplainer(model=RF_model, data=X)\n",
    "shap_values = explainer.shap_values(X, check_additivity=True)\n",
    "\n",
    "# # feature_importance = np.mean(np.abs(shap_values), axis=0)\n",
    "feature_importance = np.array([np.nanmedian(shap_values[np.where(X[:,i] == 1), i]) for i in range(7)])\n",
    "feature_ranking = np.argsort(feature_importance) # ascending "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shap plot \n",
    "labels = (\"$W_{ii}$\", \"$W_{hh}$\", \"$W_{oo}$\", \"$W_{io}$\", \"$W_{oi}$\", \"$W_{hi}$\", \"$W_{oh}$\")\n",
    "colors = ((0.5, 0.5, 0.5), (1, 1, 1))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=7, figsize=(10*2,5), sharex=True, sharey=True)\n",
    "for a, i in enumerate(feature_ranking):\n",
    "    plt.sca(ax[a])\n",
    "    plt.hlines(y=0.0, xmin=-0.5, xmax=0.5, color='xkcd:grey', zorder=0)\n",
    "    sns.violinplot(\n",
    "        x=np.zeros(shap_values.shape[0]),\n",
    "        y=shap_values[:,i], \n",
    "        hue=X[:,i], \n",
    "        palette=colors, split=True, inner=None, cut=True)\n",
    "    \n",
    "    ax[a].set_xticks([])\n",
    "    plt.xlabel(labels[i])\n",
    "\n",
    "    if a == 0: \n",
    "        plt.ylabel('Shap value')\n",
    "        # plt.ylim([-0.1, 0.1])\n",
    "        # plt.ylim([-0.02, 0.02])\n",
    "    else:\n",
    "        ax[a].get_legend().remove()\n",
    "        ax[a].spines['left'].set_visible(False)           \n",
    "        ax[a].tick_params(left=False, bottom=False)\n",
    "\n",
    "    ax[a].spines['bottom'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([4, 0, 6, 5, 2, 1, 3])\n",
    "# b = np.array([0, 2, 3, 5, 4, 1, 6])\n",
    "\n",
    "# a = np.array([np.where(a == i)[0][0] for i in range(7)])\n",
    "# b = np.array([np.where(b == i)[0][0] for i in range(7)])\n",
    "\n",
    "# cols = [\"xkcd:teal blue\", \"xkcd:teal blue\", \"xkcd:teal blue\", \"xkcd:topaz\", \"xkcd:topaz\", \"xkcd:orange\",\"xkcd:orange\"]\n",
    "# labels = (\"$W_{ii}$\", \"$W_{hh}$\", \"$W_{oo}$\", \"$W_{io}$\", \"$W_{oi}$\", \"$W_{hi}$\", \"$W_{oh}$\")\n",
    "# alphas = [1.0, 0.75, 0.5, 1.0, 0.5, 1.0, 0.5]\n",
    "\n",
    "# for i in range(7):\n",
    "#     plt.plot([0, 1], [a[i], b[i]], c=cols[i], alpha=alphas[i], label=labels[i]);\n",
    "\n",
    "# plt.xticks([0,1], [\"Fitness\", \"Robustness\"])\n",
    "# plt.ylabel('Ranking')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple noise levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import shap\n",
    "\n",
    "shap_values = []\n",
    "for a in tqdm(range(results.shape[0])): # for each level of noise\n",
    "    \n",
    "    # Structure \n",
    "    # X = np.transpose(wm_flags, (0, 2, 1)).reshape(-1,7)\n",
    "\n",
    "    # Capacity (for test fitness)\n",
    "    X = np.zeros((len(auc.reshape(-1)), 3)) * np.nan\n",
    "    X[:,0] = training_fs_n_auc.reshape(-1) # training capacity \n",
    "    X[:,1] = input_sens_test_mean.reshape(-1) # input sensitivity \n",
    "    X[:,2] = np.sum(memories[:,2, 1:], axis=1).reshape(-1) # memory capacity \n",
    "\n",
    "    y = results[a,:,:].reshape(-1) # accuracy \n",
    "\n",
    "    X = X[np.isnan(y) == False]\n",
    "    y = y[np.isnan(y) == False]\n",
    "\n",
    "    assert len(X) == len(y), \"Mismatch\"\n",
    "\n",
    "    # Fit random forest model\n",
    "    RF_model = RandomForestRegressor()\n",
    "    RF_model.fit(X,y)\n",
    "\n",
    "    # Shap analysis\n",
    "    explainer = shap.TreeExplainer(model=RF_model, data=X)\n",
    "    s_v = explainer.shap_values(X, check_additivity=True) # networks x features\n",
    "\n",
    "    shap_values.append(np.copy(s_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_summary = np.zeros((len(shap_values), shap_values[0].shape[1])) * np.nan # noises x features\n",
    "for a in range(shap_summary.shape[0]):\n",
    "    for b in range(shap_summary.shape[1]):\n",
    "\n",
    "        if X.shape[1] == 3: \n",
    "            shap_summary[a,b] = np.nanmean(shap_values[a][shap_values[a][:,b] > 0, b])\n",
    "        elif X.shape[1] == 7: \n",
    "            shap_summary[a,b] = np.nanmean(shap_values[a][X[:,b] == 1, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot\n",
    "if X.shape[1] == 3: \n",
    "    labels = (\"Tc\", \"Ic\", \"Mc\")\n",
    "    cols = [\"k\", \"k\", \"k\"]\n",
    "    alphas = [1.0, 0.75, 0.5]\n",
    "    \n",
    "elif X.shape[1] == 7:\n",
    "    labels = (\"L0\", \"L1\", \"L2\", \"S0\", \"S1\", \"B0\", \"B1\")\n",
    "    cols = [\"xkcd:teal blue\", \"xkcd:teal blue\", \"xkcd:teal blue\", \"xkcd:topaz\", \"xkcd:topaz\", \"xkcd:orange\",\"xkcd:orange\"]\n",
    "    alphas = [1.0, 0.75, 0.5, 1.0, 0.5, 1.0, 0.5]\n",
    "\n",
    "ylim = [np.min(shap_summary) * 1.1, np.max(shap_summary) * 1.1]\n",
    "plt.vlines(x=0.05, ymin=ylim[0], ymax=ylim[1], linestyles=\":\", color='k', alpha=0.5, label='Training noise')\n",
    "for a in range(shap_summary.shape[1]):\n",
    "    plt.plot(noises, shap_summary[:,a], color=cols[a], alpha=alphas[a], label=labels[a])\n",
    "\n",
    "plt.xlabel('Sensor noise')\n",
    "plt.ylabel('Shap value')\n",
    "plt.ylim(ylim)\n",
    "\n",
    "plt.legend(loc=\"upper center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dowhy import CausalModel\n",
    "import pandas as pd \n",
    "\n",
    "def min_max_norm(x): \n",
    "    return (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "# Remove stateless networks\n",
    "results[2][np.nanmedian(results[2], axis=1) < 0.5] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure\n",
    "X = np.transpose(wm_flags, (0, 2, 1)).reshape(-1,7) \n",
    "Z = np.repeat(np.arange(129), repeats=results.shape[2])\n",
    "\n",
    "# Capacity (for test accuracy)\n",
    "C = np.zeros((X.shape[0], 3)) * np.nan\n",
    "C[:,0] = min_max_norm(training_fs_n_auc.reshape(-1)) # training capacity \n",
    "C[:,1] = min_max_norm(input_sens_test_mean.reshape(-1)) # input sensitivity \n",
    "C[:,2] = min_max_norm(np.sum(memories[:,2, 1:], axis=1).reshape(-1)) # memory capacity \n",
    "\n",
    "# # Capacity (for robustness to noise)\n",
    "# C = np.zeros((X.shape[0], 3)) * np.nan\n",
    "# C[:,0] = min_max_norm(training_fs_n_auc.reshape(-1)) # training capacity \n",
    "# C[:,1] = min_max_norm(input_sens_overall_mean.reshape(-1)) # input sensitivity \n",
    "# C[:,2] = min_max_norm(np.nanmean(np.sum(memories[:,:,1:], axis=2), axis=1).reshape(-1)) # memory capacity \n",
    "\n",
    "# Function \n",
    "y = results[2,:,:].reshape(-1) # test accuracy \n",
    "# y = auc.reshape(-1) # robustness to noise \n",
    "\n",
    "# Remove nan values \n",
    "X = X[np.isnan(y) == False]\n",
    "C = C[np.isnan(y) == False]\n",
    "Z = Z[np.isnan(y) == False]\n",
    "y = y[np.isnan(y) == False]\n",
    "assert len(X) == len(C) == len(y), \"Mismatch\"\n",
    "\n",
    "# Format data \n",
    "data = pd.DataFrame(X, columns= [\"L0\", \"L1\", \"L2\", \"S0\", \"S1\", \"B0\", \"B1\"])\n",
    "for a, l in enumerate(['Tc', 'Ic', 'Mc']):\n",
    "    data[l] = C[:,a]\n",
    "data['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots\n",
    "plt.scatter(C[:,2], y, s=1, c='k', alpha=0.1)\n",
    "\n",
    "interest = [1,128, np.argmax(np.nanmedian(results[2,:,:], axis=1)), np.argmax(np.nanmedian(auc, axis=1))]\n",
    "i_cols = ['xkcd:dusty blue', 'xkcd:purple', 'xkcd:dark seafoam', 'xkcd:yellowish']\n",
    "i_labels = ['Feedforward (L)', 'Fully recurrent', 'Most accurate', 'Most robust']\n",
    "\n",
    "# for a, i in enumerate(interest):\n",
    "#     plt.scatter(C[Z==i, 1], y[Z==i], c=i_cols[a], s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='dowhy')\n",
    "warnings.filterwarnings('ignore', module='statsmodel')\n",
    "\n",
    "CI = np.zeros((11,11)) * np.nan\n",
    "for a, t in enumerate(tqdm(data.columns.to_list())):\n",
    "    for b, o in enumerate(data.columns.to_list()):\n",
    "        model = CausalModel(\n",
    "            data=data,\n",
    "            treatment=t,\n",
    "            outcome=o,\n",
    "            graph=\"digraph { \"\n",
    "                \"L0 -> y; L1 -> y; L2 -> y; S0 -> y; S1 -> y; B0 -> y; B1 -> y; Tc -> y; Ic -> y; Mc -> y;\"\n",
    "                \"L0 -> Tc; L1 -> Tc; L2 -> Tc; S0 -> Tc; S1 -> Tc; B0 -> Tc; B1 -> Tc;\"\n",
    "                \"L0 -> Ic; L1 -> Ic; L2 -> Ic; S0 -> Ic; S1 -> Ic; B0 -> Ic; B1 -> Ic;\"\n",
    "                \"L0 -> Mc; L1 -> Mc; L2 -> Mc; S0 -> Mc; S1 -> Mc; B0 -> Mc; B1 -> Mc}\"\n",
    "        )   \n",
    "        # model.view_model(layout=\"dot\")\n",
    "\n",
    "        identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)\n",
    "\n",
    "        estimate = model.estimate_effect(identified_estimand,\n",
    "                method_name=\"backdoor.linear_regression\")\n",
    "\n",
    "        try:\n",
    "            ci = estimate.get_confidence_intervals(confidence_level=0.95)\n",
    "            r = model.refute_estimate(identified_estimand, estimate, method_name=\"data_subset_refuter\")\n",
    "\n",
    "            if (np.prod(ci) > 0) & (r.refutation_result['is_statistically_significant'] == False): \n",
    "                CI[a,b] = estimate.value\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "# 12mins to run for a single experiment, 40 mins for 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nanmin(CI), np.nanmax(CI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "import matplotlib\n",
    "cmap = plt.cm.get_cmap('PiYG')\n",
    "# norm = matplotlib.colors.Normalize(vmin=np.nanmax(abs(CI))*-1, vmax=np.nanmax(abs(CI)))\n",
    "norm = matplotlib.colors.Normalize(vmin=-0.05, vmax=0.05)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Nodes \n",
    "plt.scatter(x=np.zeros(7), y=range(7), c='xkcd:light grey', zorder=1)\n",
    "plt.scatter(x=np.ones(3), y=[2,3,4],c='xkcd:light grey', zorder=1)\n",
    "plt.scatter(x=2, y=3, c='xkcd:light grey', zorder=1)\n",
    "\n",
    "# Edges \n",
    "for i in range(7):\n",
    "\n",
    "    # X -> C\n",
    "    for o in [2,3,4]:\n",
    "        try: \n",
    "            plt.plot([0,1], [i,o], c=cmap(norm(CI[i,o+5])), zorder=0)\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "    # X -> Y \n",
    "    if (i > 2) & (i < 5): \n",
    "        a = 0.25\n",
    "    elif (i ==2):\n",
    "        a = -0.25\n",
    "    else: \n",
    "        a = 0.0\n",
    "    plt.plot([0,1], [i, i+a], c=cmap(norm(CI[i,-1])), zorder=0)\n",
    "    plt.plot([1,2], [i+a, 3], c=cmap(norm(CI[i,-1])), zorder=0)\n",
    "\n",
    "# C -> Y \n",
    "for i in [2,3,4]:\n",
    "    plt.plot([1, 2], [i,3], c=cmap(norm(CI[i+5,-1])), zorder=0)\n",
    "\n",
    "# plt.yticks(range(7), labels=[\"L0\", \"L1\", \"L2\", \"S0\", \"S1\", \"B0\", \"B1\"])\n",
    "plt.yticks(range(7), labels=[\"$W_{ii}$\", \"$W_{hh}$\", \"$W_{oo}$\", \"$W_{io}$\", \"$W_{oi}$\", \"$W_{hi}$\", \"$W_{oh}$\"])\n",
    "\n",
    "plt.xticks([0,1,2], labels=[\"\", 'Tc, Ic, Mc', 'Fitness'], rotation=90)\n",
    "\n",
    "ax.spines[['left', 'bottom']].set_visible(False)\n",
    "ax.tick_params(left=False, bottom=False)\n",
    "\n",
    "cb = fig.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "             ax=ax, orientation='vertical', label='Causal effect', shrink=0.5)\n",
    "cb.outline.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['../Results/test' + str(n) + '/' for n in range(826,851)]\n",
    "print(paths)\n",
    "\n",
    "interest = [1, 128]\n",
    "i_cols = ['xkcd:dusty blue', 'xkcd:purple']\n",
    "i_labels = ['Feedforward (L)', 'Fully recurrent']\n",
    "\n",
    "agents, idxs = [], []\n",
    "for a, path in enumerate(tqdm(paths)):\n",
    "\n",
    "    # Load data \n",
    "    for f in interest:\n",
    "        try:\n",
    "            with open(path + str(f) + \".pickle\", 'rb') as file:\n",
    "                    agnt = pickle.load(file)\n",
    "                    agents.append(agnt)\n",
    "                    idxs.append(f)\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "print(np.unique(idxs, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity (Jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "from scipy.optimize import curve_fit\n",
    "import copy \n",
    "\n",
    "def gaussian(x, amp, mean, stddev):\n",
    "    return amp * np.exp(-((x - mean) ** 2) / (2 * stddev ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sensitivity\n",
    "import seaborn as sns\n",
    "\n",
    "sensor_noise_scale = 0.05 \n",
    "n_steps = 6 \n",
    "\n",
    "maze = multimodal_mazes.TrackMaze(size=11, n_channels=2)\n",
    "maze.generate(number=1000, noise_scale=0, gaps=2)\n",
    "\n",
    "popts = []\n",
    "for a, i in enumerate(tqdm(interest)): # for each architecture\n",
    "    for _, v in enumerate(np.where(np.array(idxs) == i)[0]): # for each network\n",
    "        agnt = copy.deepcopy(agents[v])\n",
    "        all_states = []        \n",
    "\n",
    "        # Collect states\n",
    "        _, all_states = multimodal_mazes.eval_fitness(\n",
    "            genome=None, config=None, channels=agnt.channels, \n",
    "            sensor_noise_scale=sensor_noise_scale, drop_connect_p=0.0, \n",
    "            maze=maze, n_steps=n_steps, agnt=agnt, record_states=True)\n",
    "\n",
    "        # Calculate jacobian norms    \n",
    "        xs, ys = multimodal_mazes.calculate_dqn_input_sensitivity(all_states, agnt)\n",
    "        idx = np.argsort(xs)\n",
    "\n",
    "        # Fit curve\n",
    "        try:\n",
    "            popt, _ = curve_fit(gaussian, xs[idx], ys[idx], bounds=((-np.inf, -np.inf, 0), (np.inf, np.inf, np.inf)))\n",
    "            y_fit = gaussian(xs[idx], *popt)\n",
    "            popts.append(popt)\n",
    "\n",
    "            # Plot curve\n",
    "            plt.plot(xs[idx], y_fit, color=i_cols[a], alpha=0.75)\n",
    "        except:\n",
    "            popts.append(np.array([np.nan, np.nan, np.nan]))\n",
    "\n",
    "plt.xticks(ticks=[-1, 0, 1], labels=(\"-1.0\", \"0.0\", \"1.0\"))\n",
    "plt.xlabel('(L-R)/(L+R)')\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "\n",
    "for a, i in enumerate(interest):\n",
    "    plt.plot([], [], color=i_cols[a], alpha=0.75, label=i_labels[a])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.repeat([0,1,2,3], repeats=10)\n",
    "for l in [0, 1,2,3]:\n",
    "    print(np.round(np.nanmean(np.array(popts)[labels==l], axis=0),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, popt in enumerate(popts):\n",
    "    if labels[a] != 0:\n",
    "        plt.scatter(labels[a], popt[2], color=i_cols[labels[a]], alpha=0.75)\n",
    "\n",
    "plt.xlabel('Architectures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_noise_scale = 0.05 \n",
    "n_steps = 6 \n",
    "\n",
    "maze = multimodal_mazes.TrackMaze(size=11, n_channels=2)\n",
    "maze.generate(number=1000, noise_scale=0, gaps=2)\n",
    "\n",
    "fitness = np.zeros((len(agents), 8)) * np.nan\n",
    "\n",
    "for a in tqdm(range((len(agents)))): # for each network\n",
    "    wm_idx = [0] + list(np.where(agents[a].wm_flags)[0] + 1)\n",
    "    for b, wm in enumerate([np.inf] + list(np.arange(2, len(list(agents[a].parameters()))))): # for each weight matrix\n",
    "        agnt = copy.deepcopy(agents[a])\n",
    "        for c, param in enumerate(agnt.parameters()):\n",
    "            if c == wm: \n",
    "                param.data = torch.zeros(*param.shape)\n",
    "\n",
    "        fitness[a, wm_idx.pop(0)] = multimodal_mazes.eval_fitness(genome=None, config=None, channels=[1,1], sensor_noise_scale=sensor_noise_scale, drop_connect_p=0.0, maze=maze, n_steps=n_steps, agnt=agnt)\n",
    "\n",
    "fitness_norm = fitness - fitness[:,0][:,None]\n",
    "\n",
    "# np.save(\"./fitness\", fitness)\n",
    "# np.save(\"./fitness_norm\", fitness_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness = np.load(\"./fitness.npy\")\n",
    "fitness_norm = np.load(\"./fitness_norm.npy\")\n",
    "\n",
    "interest = [1, 128, 74, 36]\n",
    "i_cols = ['xkcd:dusty blue', 'xkcd:purple', 'xkcd:dark seafoam', 'xkcd:yellowish']\n",
    "i_labels = ['Feedforward (L)', 'Fully recurrent', 'Most accurate', 'Most robust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=np.tile(np.arange(0,8), reps=fitness.shape[0]), y=fitness.reshape(-1), color='k', marker='.', alpha=0.5)\n",
    "plt.ylim([-0.05, 1.05])\n",
    "\n",
    "for a, i in enumerate(interest):\n",
    "    plt.scatter(x=np.tile(np.arange(0,8), reps=10), y=fitness[np.array(idxs) == i].reshape(-1), color=i_cols[a], marker='.', alpha=0.5)\n",
    "\n",
    "plt.xticks(ticks=np.arange(8), labels=(\"N\", \"L0\", \"L1\", \"L2\", \"S0\", \"S1\", \"B0\", \"B1\"))\n",
    "plt.xlabel('Ablation')\n",
    "plt.ylabel(\"Fitness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x=range(8),y=np.nanmean(fitness_norm, axis=0), yerr=np.nanstd(fitness_norm, axis=0), color='k', ls='None', marker='o')\n",
    "plt.xticks(ticks=np.arange(8), labels=(\"N\", \"L0\", \"L1\", \"L2\", \"S0\", \"S1\", \"B0\", \"B1\"));\n",
    "plt.xlabel('Ablation')\n",
    "plt.ylabel(\"Change in fitness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = np.linspace(start=-0.1, stop=0.1, num=4)\n",
    "for a, i in enumerate(interest):\n",
    "    plt.errorbar(\n",
    "        x=range(8) + np.repeat(offsets[a], repeats=8),\n",
    "        y=np.nanmean(fitness_norm[np.array(idxs) == i], axis=0), \n",
    "        yerr=np.nanstd(fitness_norm[np.array(idxs) == i], axis=0), \n",
    "        color=i_cols[a], ls='None', marker='o')\n",
    "\n",
    "plt.xticks(ticks=np.arange(8), labels=(\"N\", \"L0\", \"L1\", \"L2\", \"S0\", \"S1\", \"B0\", \"B1\"));\n",
    "plt.xlabel('Ablation')\n",
    "plt.ylabel(\"Change in fitness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP: \n",
    "\n",
    "# Collect data \n",
    "exp_config = multimodal_mazes.load_exp_config(\"../exp_config.ini\")\n",
    "noises = np.linspace(start=0.0, stop=0.5, num=11)\n",
    "\n",
    "maze = multimodal_mazes.TrackMaze(size=11, n_channels=2)\n",
    "maze.generate(number=1000, noise_scale=0, gaps=1)\n",
    "\n",
    "results = []\n",
    "v = -1\n",
    "agnt = copy.deepcopy(agents[v])\n",
    "\n",
    "# Test\n",
    "results, input_sensitivity, memory = multimodal_mazes.test_dqn_agent(\n",
    "    maze_test=maze,\n",
    "    agnt=agnt,\n",
    "    exp_config=exp_config,\n",
    "    noises=noises,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As partial derivatives \n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "def forward(*states):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        states: a tuple storing tensors of:\n",
    "            inputs, prev_inputs, hidden, prev_outputs, outputs\n",
    "            from multiple time points. \n",
    "            I.e. inputs occur every 5th tensor. \n",
    "    Returns:\n",
    "        agnt.outputs: output activations at t.\n",
    "    \"\"\"\n",
    "\n",
    "    for t, input in enumerate(states[::5]):\n",
    "\n",
    "        agnt.channel_inputs = input\n",
    "\n",
    "        if t == 0:\n",
    "            agnt.outputs, prev_input, hidden, prev_output = agnt.forward(\n",
    "                states[1], states[2], states[3], tensor_input=True)\n",
    "        else: \n",
    "             agnt.outputs, prev_input, hidden, prev_output = agnt.forward(\n",
    "                prev_input, hidden, prev_output, tensor_input=True)\n",
    "            \n",
    "    return agnt.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data \n",
    "sensor_noise_scale = 0.05\n",
    "n_steps = 6 \n",
    "\n",
    "maze = multimodal_mazes.TrackMaze(size=11, n_channels=2)\n",
    "maze.generate(number=1000, noise_scale=0, gaps=1)\n",
    "\n",
    "results = []\n",
    "for v in tqdm(range(len(agents))):\n",
    "    agnt = copy.deepcopy(agents[v])\n",
    "\n",
    "    # Fitness\n",
    "    _, all_states = multimodal_mazes.eval_fitness(\n",
    "        genome=None,\n",
    "        config=None,\n",
    "        channels=[1,1],\n",
    "        sensor_noise_scale=sensor_noise_scale,\n",
    "        drop_connect_p=0.0,\n",
    "        maze=maze,\n",
    "        n_steps=n_steps,\n",
    "        agnt=copy.deepcopy(agnt),\n",
    "        record_states=True,\n",
    "    )\n",
    "\n",
    "    # Calculate \n",
    "    memory = [[] for _ in range(n_steps)]\n",
    "\n",
    "    for a in range(len(all_states)): # for each trial\n",
    "\n",
    "        trial_states = tuple(itertools.chain.from_iterable(all_states[a])) # a tuple of tensor states\n",
    "\n",
    "        for b, _ in enumerate(trial_states[::5]): # for each time point \n",
    "            \n",
    "            jm = jacobian(forward, trial_states[:(5 * (b+1))]) # a tuple of Jacobians (one per state)\n",
    "\n",
    "            for c, j in enumerate(jm[::5][::-1]): # for each (sensor) input state (from t backwards)\n",
    "                memory[c].append(torch.norm(j, p=\"fro\") / torch.norm(jm[::5][-1], p=\"fro\")) # append the norm divided by the norm at time t\n",
    "\n",
    "    # Store \n",
    "    tmp = []\n",
    "    for i in range(n_steps):\n",
    "        memory[i] = np.array(memory[i])\n",
    "        memory[i][np.isinf(memory[i])] = np.nan\n",
    "        tmp.append(np.nanmean(memory[i]))\n",
    "    results.append(tmp)\n",
    "\n",
    "results = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure \n",
    "plt.plot(results[np.array(idxs)==1, ::-1].T, c=i_cols[0], alpha=0.75);\n",
    "plt.plot(results[np.array(idxs)==128, ::-1].T, c=i_cols[1], alpha=0.75);\n",
    "plt.hlines(y=1.0, xmin=0, xmax=5, color='k', alpha=0.25)\n",
    "\n",
    "plt.xticks(np.arange(n_steps), labels=np.arange(n_steps)[::-1]*-1)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Input-output influence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As estimated MI \n",
    "\n",
    "# Calculating sensitivity\n",
    "sensor_noise_scale = 0.05\n",
    "n_steps = 6 \n",
    "\n",
    "maze = multimodal_mazes.TrackMaze(size=11, n_channels=2)\n",
    "maze.generate(number=1000, noise_scale=0, gaps=1)\n",
    "\n",
    "mi_norms = [[] for _ in interest]\n",
    "for a, i in enumerate(tqdm(interest)): # for each architecture\n",
    "    for _, v in enumerate(np.where(np.array(idxs) == i)[0]): # for each network\n",
    "        agnt = copy.deepcopy(agents[v])\n",
    "        all_states = []        \n",
    "\n",
    "        # Collect states\n",
    "        _, all_states = multimodal_mazes.eval_fitness(\n",
    "            genome=None, config=None, channels=agnt.channels, \n",
    "            sensor_noise_scale=sensor_noise_scale, drop_connect_p=0.0, \n",
    "            maze=maze, n_steps=n_steps, agnt=agnt, record_states=True)\n",
    "\n",
    "        # Estimate memory\n",
    "        mis = multimodal_mazes.estimate_dqn_memory(all_states=all_states, agnt=agnt, n_steps=n_steps)\n",
    "\n",
    "        mi_norms[a].append(mis)\n",
    "\n",
    "for a, i in enumerate(interest):\n",
    "    mi_norms[a] = np.array(mi_norms[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "for a, i in enumerate(interest):    \n",
    "    x = range(mi_norms[a].shape[1])\n",
    "    y = np.nanmean(mi_norms[a], axis=0)\n",
    "    sem = np.nanstd(mi_norms[a], axis=0) / np.sqrt(mi_norms[a].shape[0])\n",
    "    plt.plot(x, y, color=i_cols[a], label=i_labels[a])\n",
    "    plt.fill_between(x, y-sem, y+sem, color=i_cols[a], alpha=0.25, edgecolor=None)\n",
    "\n",
    "plt.ylim([-0.05,1.50])\n",
    "plt.legend()\n",
    "plt.xlabel('Temporal lag')\n",
    "plt.ylabel(\"Estimated MI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "for a, i in enumerate(interest):    \n",
    "    plt.scatter(np.ones(10) * a, np.sum(mi_norms[a], axis=1), color=i_cols[a])\n",
    "\n",
    "plt.xlabel('Temporal lag')\n",
    "plt.ylabel(\"Estimated MI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First pass \n",
    "\n",
    "L_input = [0.1,0.1]\n",
    "R_inputs = [[0.1, 0.0], [0.0, 0.1]]\n",
    "\n",
    "for a, i in enumerate(interest): # for each architecture \n",
    "    L_outputs = []\n",
    "    for i_num, v in enumerate(np.where(np.array(idxs) == i)[0]): # for each network\n",
    "        agnt = copy.deepcopy(agents[v])\n",
    "\n",
    "        for b in range(2):\n",
    "            l_outputs = [] \n",
    "            \n",
    "            # Reset agent\n",
    "            agnt.prev_input *= 0.0\n",
    "            agnt.hidden *= 0.0\n",
    "            agnt.prev_output *= 0.0\n",
    "            agnt.outputs *= 0.0\n",
    "            agnt.channel_inputs *= 0.0    \n",
    "\n",
    "            for t in range(6):\n",
    "\n",
    "                if t == 0:\n",
    "                    agnt.channel_inputs[0] = np.copy(L_input)\n",
    "                    agnt.channel_inputs[1] = np.copy(R_inputs[b])\n",
    "                else: \n",
    "                    agnt.channel_inputs *= 0.0\n",
    "\n",
    "                agnt.policy()\n",
    "                l_outputs.append(np.copy(agnt.outputs[0]))\n",
    "            L_outputs.append(l_outputs)\n",
    "\n",
    "    x = range(np.array(L_outputs).shape[1])\n",
    "    y = np.nanmean(np.array(L_outputs),axis=0)\n",
    "    sem = np.nanstd(np.array(L_outputs),axis=0) / np.sqrt(i_num + 1)\n",
    "    plt.plot(x, y, color=i_cols[a], label=i_labels[a])\n",
    "    plt.fill_between(x, y-sem, y+sem, color=i_cols[a], alpha=0.25, edgecolor=None)\n",
    "\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.legend()\n",
    "plt.xlabel('Time since input')\n",
    "plt.ylabel(\"p(Left)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, i in enumerate(interest): # for each architecture \n",
    "    L_outputs = []\n",
    "    for i_num, v in enumerate(np.where(np.array(idxs) == i)[0]): # for each network\n",
    "        agnt = copy.deepcopy(agents[v])\n",
    "\n",
    "        for _ in range(100):\n",
    "            l_outputs = [] \n",
    "            \n",
    "            # Reset agent\n",
    "            agnt.prev_input *= 0.0\n",
    "            agnt.hidden *= 0.0\n",
    "            agnt.prev_output *= 0.0\n",
    "            agnt.outputs *= 0.0\n",
    "            agnt.channel_inputs *= 0.0    \n",
    "\n",
    "            for t in range(6):\n",
    "                agnt.channel_inputs = np.random.normal(\n",
    "                    loc=0.0, scale=0.05, size=agnt.channel_inputs.shape\n",
    "                )\n",
    "                agnt.channel_inputs = np.clip(agnt.channel_inputs, a_min=0.0, a_max=1.0)\n",
    "                agnt.policy()\n",
    "                l_outputs.append(np.std(np.array(agnt.outputs)))\n",
    "            L_outputs.append(l_outputs)\n",
    "\n",
    "    x = range(np.array(L_outputs).shape[1])\n",
    "    y = np.nanmean(np.array(L_outputs),axis=0)\n",
    "    sem = np.nanstd(np.array(L_outputs),axis=0) / np.sqrt(i_num + 1)\n",
    "    plt.plot(x, y, color=i_cols[a], label=i_labels[a])\n",
    "    plt.fill_between(x, y-sem, y+sem, color=i_cols[a], alpha=0.25, edgecolor=None)\n",
    "\n",
    "plt.ylim([-0.05,0.45])\n",
    "plt.legend()\n",
    "plt.xlabel('Time with noise input')\n",
    "plt.ylabel(\"Std(p(outputs))\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multimodal_mazes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
