{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To modify your Q-Learning agent into a Deep Q-Learning (DQN) agent with a neural network that has one hidden layer and optional lateral, skip, and backward connections, we can integrate a simple feed-forward neural network with PyTorch or TensorFlow. I'll show how to implement the agent using PyTorch in this case. Additionally, we will introduce lateral, skip, and backward connections in a flexible way.\n",
    "\n",
    "Here's a step-by-step breakdown of the necessary changes:\n",
    "\n",
    "### 1. Set Up the Neural Network\n",
    "You need to replace the linear function approximation (the `theta` matrix) with a neural network that estimates the Q-values for each state-action pair.\n",
    "\n",
    "### 2. Implement Replay Buffer\n",
    "DQN uses experience replay to store transitions and learn from them in a more stable way.\n",
    "\n",
    "### 3. Modify the Learning Method\n",
    "Instead of directly updating `theta`, we'll use the neural network to predict Q-values and update its weights using backpropagation and gradient descent.\n",
    "\n",
    "### 4. Optional Connections\n",
    "We'll integrate lateral, skip, and backward connections into the neural network by adding extra flexibility into the model architecture.\n",
    "\n",
    "### Code Implementation\n",
    "\n",
    "Hereâ€™s how you can modify the agent to achieve this:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, lateral=False, skip=False, backward=False):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.lateral = lateral\n",
    "        self.skip = skip\n",
    "        self.backward = backward\n",
    "\n",
    "        # Input layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Optional lateral connections (within hidden layer)\n",
    "        if lateral:\n",
    "            self.lateral_fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Optional skip connections (input directly to output)\n",
    "        if skip:\n",
    "            self.skip_fc = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Optional backward connections (output to hidden layer)\n",
    "        if backward:\n",
    "            self.backward_fc = nn.Linear(output_dim, hidden_dim)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        hidden_output = x\n",
    "        \n",
    "        if self.lateral:\n",
    "            x = torch.relu(self.lateral_fc(x))\n",
    "\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        if self.skip:\n",
    "            x += torch.relu(self.skip_fc(hidden_output))\n",
    "\n",
    "        if self.backward:\n",
    "            x += torch.relu(self.backward_fc(self.fc3(x)))\n",
    "\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=10000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNLearnerAgent:\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, buffer_size=10000, batch_size=64, lateral=False, skip=False, backward=False):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.q_network = QNetwork(input_dim, output_dim, hidden_dim, lateral, skip, backward)\n",
    "        self.target_network = QNetwork(input_dim, output_dim, hidden_dim, lateral, skip, backward)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.output_dim)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state_tensor)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "    def learn(self):\n",
    "        if self.replay_buffer.size() < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "\n",
    "        # Get current Q values\n",
    "        q_values = self.q_network(states).gather(1, actions)\n",
    "\n",
    "        # Compute target Q values\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].unsqueeze(1)\n",
    "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.criterion(q_values, target_q_values)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Epsilon decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "    def reset(self):\n",
    "        self.q_network.reset_parameters()\n",
    "        self.target_network.reset_parameters()\n",
    "```\n",
    "\n",
    "### Key Features in the New Agent\n",
    "\n",
    "1. **Neural Network (`QNetwork`):**\n",
    "   - The network has one hidden layer, and there are optional lateral, skip, and backward connections.\n",
    "   - The `lateral` flag introduces lateral connections within the hidden layer.\n",
    "   - The `skip` flag allows information to skip the hidden layer, connecting directly from input to output.\n",
    "   - The `backward` flag connects the output back to the hidden layer, allowing feedback.\n",
    "\n",
    "2. **Replay Buffer:**\n",
    "   - Stores transitions `(state, action, reward, next_state, done)` to train the network using experience replay, which stabilizes learning.\n",
    "\n",
    "3. **DQN Training (`learn` method):**\n",
    "   - Samples a batch from the replay buffer and updates the Q-network based on the target network's predictions.\n",
    "   - Gradients are computed and the network is updated using gradient descent.\n",
    "\n",
    "4. **Epsilon-Greedy Policy:**\n",
    "   - The agent uses an epsilon-greedy strategy for exploration.\n",
    "   - Epsilon decays after each episode, leading to less exploration and more exploitation over time.\n",
    "\n",
    "### Usage\n",
    "- **Acting:** The `act` method selects an action based on the current policy (exploration or exploitation).\n",
    "- **Learning:** The `learn` method updates the Q-values using the Bellman equation.\n",
    "- **Replay Buffer:** Stores transitions that the agent uses to learn during training.\n",
    "- **Neural Network Update:** The agent updates its target network periodically using the `update_target_network` method.\n",
    "\n",
    "This implementation provides a framework for Deep Q-Learning with optional lateral, skip, and backward connections, which you can toggle when initializing the agent. You can modify the neural network structure further to suit your needs or test different architectures."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
