{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN prey notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import neat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "import pickle\n",
    "import multimodal_mazes\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness vs noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "\n",
    "# Task\n",
    "exp_config = multimodal_mazes.load_prey_config(\"../prey_config.ini\")\n",
    "\n",
    "# Agent \n",
    "n_hidden_units = 8\n",
    "wm_flag = np.array([0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnt = multimodal_mazes.AgentDQN(location=None, channels=exp_config['channels'], sensor_noise_scale=exp_config['sensor_noise_scale'], n_hidden_units=n_hidden_units, wm_flags=wm_flag)\n",
    "\n",
    "# agnt.generate_predator_policy(n_train_trials=100, n_test_trials=None, exp_config=exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness, _, _, _ = multimodal_mazes.eval_predator_fitness(\n",
    "    n_trials=1000,\n",
    "    size=exp_config[\"size\"],\n",
    "    agnt=agnt,\n",
    "    sensor_noise_scale=agnt.sensor_noise_scale,\n",
    "    n_prey=exp_config[\"n_prey\"],\n",
    "    pk=exp_config[\"pk\"],\n",
    "    n_steps=exp_config[\"n_steps\"],\n",
    "    scenario=exp_config[\"scenario\"],\n",
    "    motion=exp_config[\"motion\"],\n",
    "    pc=exp_config[\"pc\"],\n",
    "    pm=exp_config[\"pm\"],\n",
    "    pe=exp_config[\"pe\"],\n",
    ")\n",
    "\n",
    "print(fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time, path, _, _, _ =  multimodal_mazes.predator_trial(\n",
    "    size=exp_config[\"size\"],\n",
    "    agnt=agnt,\n",
    "    sensor_noise_scale=agnt.sensor_noise_scale,\n",
    "    n_prey=exp_config[\"n_prey\"],\n",
    "    pk=exp_config[\"pk\"],\n",
    "    n_steps=exp_config[\"n_steps\"],\n",
    "    scenario=exp_config[\"scenario\"],\n",
    "    motion=exp_config[\"motion\"],\n",
    "    pc=exp_config[\"pc\"],\n",
    "    pm=exp_config[\"pm\"],\n",
    "    pe=exp_config[\"pe\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predator_policy(self, n_train_trials, n_test_trials, exp_config):\n",
    "        \"\"\"\n",
    "        Uses deep Q-learning to optimise model weights.\n",
    "        Arguments:\n",
    "            n_train_trials:\n",
    "            n_steps: number of simulation steps.\n",
    "            n_test_trials:\n",
    "                Used to record the agent's fitness 100 times throughout training.\n",
    "            exp_config:\n",
    "        Updates:\n",
    "            self.parameters.\n",
    "            self.training_fitness (if n_test_trials is provided).\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        gamma = 0.9\n",
    "        epsilons = np.repeat(\n",
    "            np.linspace(start=0.95, stop=0.25, num=10), repeats=n_train_trials // 10\n",
    "        )\n",
    "\n",
    "        self.gradient_norms = []\n",
    "        self.training_fitness = []\n",
    "\n",
    "        for a in range(n_train_trials):\n",
    "\n",
    "            # Record fitness\n",
    "            if (a % (n_train_trials // 100) == 0) & (n_test_trials != None):\n",
    "                with torch.no_grad():\n",
    "                    fitness, _, _, _ = multimodal_mazes.eval_predator_fitness(\n",
    "                        n_trials=n_test_trials,\n",
    "                        size=exp_config[\"size\"],\n",
    "                        agnt=self,\n",
    "                        sensor_noise_scale=self.sensor_noise_scale,\n",
    "                        n_prey=exp_config[\"n_prey\"],\n",
    "                        pk=exp_config[\"pk\"],\n",
    "                        n_steps=exp_config[\"n_steps\"],\n",
    "                        scenario=exp_config[\"scenario\"],\n",
    "                        motion=exp_config[\"motion\"],\n",
    "                        pc=exp_config[\"pc\"],\n",
    "                        pm=exp_config[\"pm\"],\n",
    "                        pe=exp_config[\"pe\"],\n",
    "                    )\n",
    "                    self.training_fitness.append(fitness)\n",
    "                    print(fitness)\n",
    "\n",
    "            pk_hw = (\n",
    "                exp_config[\"pk\"] // 2\n",
    "            )  # half width of prey's Gaussian signal (in rc)\n",
    "\n",
    "            # Reset agent\n",
    "            prev_input = torch.zeros(self.n_input_units)\n",
    "            hidden = torch.zeros(self.n_hidden_units)\n",
    "            prev_output = torch.zeros(self.n_output_units)\n",
    "\n",
    "            self.location = np.array(\n",
    "                [pk_hw + (exp_config[\"size\"] // 2), pk_hw + (exp_config[\"size\"] // 2)]\n",
    "            )\n",
    "            self.outputs = torch.zeros(self.n_output_units)\n",
    "\n",
    "            loss = 0.0\n",
    "\n",
    "            # Create environment with track (1.) and walls (0.)\n",
    "            env = np.zeros(\n",
    "                (exp_config[\"size\"], exp_config[\"size\"], len(self.channels) + 1)\n",
    "            )\n",
    "            env[:, :, -1] = 1.0\n",
    "            env = np.pad(env, pad_width=((pk_hw, pk_hw), (pk_hw, pk_hw), (0, 0)))\n",
    "\n",
    "            # Define prey\n",
    "            k1d = signal.windows.gaussian(exp_config[\"pk\"], std=1)\n",
    "            k2d = np.outer(k1d, k1d)\n",
    "            k2d_noise = np.copy(k2d)\n",
    "\n",
    "            rcs = np.stack(np.argwhere(env[:, :, -1]))\n",
    "            prey_rcs = np.random.choice(\n",
    "                range(len(rcs)), size=exp_config[\"n_prey\"], replace=False\n",
    "            )\n",
    "            preys = []\n",
    "            for n in range(exp_config[\"n_prey\"]):\n",
    "                preys.append(\n",
    "                    multimodal_mazes.AgentRandom(\n",
    "                        location=rcs[prey_rcs[n]],\n",
    "                        channels=[0, 0],\n",
    "                        motion=exp_config[\"motion\"],\n",
    "                    )\n",
    "                )\n",
    "                preys[n].state = 1  # free (1) or caught (0)\n",
    "                preys[n].path = [list(preys[n].location)]\n",
    "\n",
    "                if exp_config[\"scenario\"] == \"Foraging\":\n",
    "                    preys[n].cues = n % 2  # channel for emitting cues\n",
    "\n",
    "            # Trial\n",
    "            prey_counter = np.copy(exp_config[\"n_prey\"])\n",
    "            for time in range(exp_config[\"n_steps\"]):\n",
    "\n",
    "                env[:, :, :-1] *= exp_config[\"pc\"]  # reset channels\n",
    "\n",
    "                # Prey\n",
    "                for prey in preys:\n",
    "                    if prey.state == 1:\n",
    "                        if (prey.location == self.location).all():  # caught\n",
    "                            prey.state = 0\n",
    "                            prey_counter -= 1\n",
    "\n",
    "                        else:  # free\n",
    "                            # Movement\n",
    "                            if exp_config[\"scenario\"] == \"Hunting\":\n",
    "                                if np.random.rand() < exp_config[\"pm\"]:\n",
    "                                    prey.policy()\n",
    "                                    prey.act(env)\n",
    "                            prey.path.append(list(prey.location))\n",
    "\n",
    "                            # Emit cues\n",
    "                            r, c = prey.location\n",
    "                            if exp_config[\"scenario\"] == \"Foraging\":\n",
    "                                env[\n",
    "                                    r - pk_hw : r + pk_hw + 1,\n",
    "                                    c - pk_hw : c + pk_hw + 1,\n",
    "                                    prey.cues,\n",
    "                                ] += np.copy(k2d)\n",
    "\n",
    "                            elif exp_config[\"scenario\"] == \"Hunting\":\n",
    "                                for ch in range(len(prey.channels)):\n",
    "                                    if np.random.rand() < exp_config[\"pe\"]:\n",
    "                                        env[\n",
    "                                            r - pk_hw : r + pk_hw + 1,\n",
    "                                            c - pk_hw : c + pk_hw + 1,\n",
    "                                            ch,\n",
    "                                        ] += np.copy(\n",
    "                                            k2d\n",
    "                                        )  # emit cues\n",
    "                                    else:\n",
    "                                        np.random.shuffle(k2d_noise.reshape(-1))\n",
    "                                        env[\n",
    "                                            r - pk_hw : r + pk_hw + 1,\n",
    "                                            c - pk_hw : c + pk_hw + 1,\n",
    "                                            ch,\n",
    "                                        ] += np.copy(\n",
    "                                            k2d_noise\n",
    "                                        )  # emit noise\n",
    "\n",
    "                # Apply edges\n",
    "                for ch in range(len(self.channels)):\n",
    "                    env[:, :, ch] *= env[:, :, -1]\n",
    "\n",
    "                # If all prey have been caught\n",
    "                if prey_counter == 0:\n",
    "                    break\n",
    "\n",
    "                # Predator\n",
    "                # Sense\n",
    "                self.sense(env)\n",
    "\n",
    "                # Epsilon-greedy action selection\n",
    "                if torch.rand(1) < epsilons[a]:\n",
    "                    action = torch.randint(\n",
    "                        low=0, high=self.n_output_units, size=(1,)\n",
    "                    ).item()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        q_values, _, _, _ = self.forward(\n",
    "                            prev_input, hidden, prev_output\n",
    "                        )\n",
    "                        action = torch.argmax(q_values).item()\n",
    "\n",
    "                # Predicted Q-value\n",
    "                q_values, prev_input, hidden, prev_output = self.forward(\n",
    "                    prev_input.detach(), hidden.detach(), prev_output.detach()\n",
    "                )\n",
    "                predicted = q_values[action]\n",
    "\n",
    "                # Act\n",
    "                self.outputs *= 0.0\n",
    "                self.outputs[action] = 1.0\n",
    "                self.act(env)\n",
    "\n",
    "                # Reward\n",
    "                reward = np.sum(self.channel_inputs) / self.channel_inputs.size\n",
    "\n",
    "                # Target Q-value\n",
    "                self.sense(env)\n",
    "                with torch.no_grad():\n",
    "                    next_q_values, _, _, _ = self.forward(\n",
    "                        prev_input, hidden, prev_output\n",
    "                    )\n",
    "                    target = reward + (gamma * torch.max(next_q_values)) - 0.1\n",
    "\n",
    "                # Loss\n",
    "                loss = loss + criterion(predicted, target)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), 10)\n",
    "\n",
    "            # Check for exploding gradients\n",
    "            # with torch.no_grad():\n",
    "            #     total_norm = 0\n",
    "            #     for p in self.parameters():\n",
    "            #         param_norm = p.grad.data.norm(2)\n",
    "            #         total_norm += param_norm.item() ** 2\n",
    "            #     total_norm = total_norm ** (1.0 / 2)\n",
    "            #     self.gradient_norms.append(total_norm)\n",
    "\n",
    "            optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multimodal_mazes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
