{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import patches\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {\n",
    "    0: {\"name\":\"Up\", \"label\":\"↑\", \"delta\":(-1,0), 'angle': float(np.pi)},\n",
    "    1: {\"name\":\"Right\", \"label\":\"→\", \"delta\":(0,1), 'angle': float(3 * np.pi / 2)},\n",
    "    2: {\"name\":\"Down\", \"label\":\"↓\", \"delta\":(1,0), 'angle': float(0.0)},\n",
    "    3: {\"name\":\"Left\", \"label\":\"←\", \"delta\":(0,-1), 'angle': float(np.pi / 2)},\n",
    "}\n",
    "\n",
    "color_dict = {\n",
    "    '0': colors.LinearSegmentedColormap.from_list(\"\", [\"white\", \"xkcd:ultramarine\"]),\n",
    "    '1': colors.LinearSegmentedColormap.from_list(\"\", [\"white\", \"xkcd:magenta\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridPlotter:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    def plot_env(self, ax=None):\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(0.1 * self.agent.width, 0.1 * self.agent.height))\n",
    "        ax.imshow((color_dict['0'](self.agent.env[:,:,0]) + color_dict['1'](self.agent.env[:,:,1]))/2, interpolation='gaussian', zorder=0) \n",
    "        ax.imshow(1 - self.agent.env, cmap=cm.binary, alpha=0.25, zorder=1)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        ax.set_xlim([self.agent.pk_hw - 1, self.agent.width - self.agent.pk_hw])\n",
    "        ax.set_ylim([self.agent.height - self.agent.pk_hw, self.agent.pk_hw - 1])\n",
    "        return ax \n",
    "    \n",
    "    def plot_prey(self, ax):\n",
    "        for prey_location in self.agent.prey_locations:\n",
    "            prey_patch = patches.Circle((prey_location[1] - 0.5, prey_location[0] - 0.5), 1, color='black', zorder=2)\n",
    "            ax.add_patch(prey_patch)\n",
    "        return ax\n",
    "\n",
    "    def plot_agent(self, ax, agent_location, color):\n",
    "        agent_patch = patches.Rectangle((agent_location[1] - 0.5, agent_location[0] - 0.5), 1, 1, color=color, zorder=3)\n",
    "        ax.add_patch(agent_patch)\n",
    "        return ax\n",
    "\n",
    "    def plot_episode(self, trial_data, ax=None):\n",
    "        agent_path = trial_data[\"path\"]\n",
    "        \n",
    "        if ax is None:\n",
    "            ax = self.plot_env()\n",
    "            ax = self.plot_rewards(ax)\n",
    "            \n",
    "        ax.set_title(f'{len(agent_path)} Steps')\n",
    "\n",
    "        for i in range(len(agent_path)):\n",
    "            trial_frac = i // self.agent.n_steps\n",
    "            ax = self.plot_agent(ax, agent_location=agent_path[i], color=cm.vidris(trial_frac))\n",
    "        return ax\n",
    "\n",
    "    def plot_training_progress(self, trial_lengths, ax=None):\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "        n_trials = len(trial_lengths)\n",
    "        smoothed_episode_lengths = [np.mean(trial_lengths[max(0, i - 100):i + 1]) for i in range(n_trials)]\n",
    "        ax.scatter(np.arange(n_trials), trial_lengths, linewidth=0, alpha=0.5, c='C0', label=\"Episode length\")\n",
    "        ax.plot(np.arange(len(smoothed_episode_lengths)), smoothed_episode_lengths, color='k', linestyle=\"--\", linewidth=0.5, label=\"Smoothed\")\n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.set_ylabel(\"Length\")\n",
    "        ax.legend()\n",
    "        ax.set_title(\"Training Progress\")\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearnerAgent:\n",
    "    def __init__(self, pk, pk_hw, channels, actions, location, sensor_noise_scale, n_steps, n_features, cost_per_step, cost_per_collision, alpha, epsilon, gamma):\n",
    "        self.pk = pk\n",
    "        self.pk_hw = pk_hw\n",
    "        self.channels = np.array(channels)\n",
    "        self.actions = actions\n",
    "        self.location = np.array(location)\n",
    "        self.sensor_noise_scale = sensor_noise_scale\n",
    "        self.n_steps = n_steps\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.n_actions = len(actions)\n",
    "        self.cost_per_step = cost_per_step\n",
    "        self.cost_per_collision = cost_per_collision\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.theta = np.zeros((n_features, self.n_actions))\n",
    "        self.plotter = GridPlotter(self)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.agent_direction = int(0)\n",
    "        \n",
    "    def sense_features(self, location):\n",
    "        features = np.zeros(self.n_features)\n",
    "        distance, angle = self.closest_prey_features(location)\n",
    "        features[0] = self.env[location[0] - 1, location[1], 0]\n",
    "        features[1] = self.env[location[0] + 1, location[1], 0]\n",
    "        features[2] = self.env[location[0], location[1] - 1, 0]\n",
    "        features[3] = self.env[location[0], location[1] + 1, 0]\n",
    "        features[4] = distance\n",
    "        features[5] = angle\n",
    "        return features\n",
    "    \n",
    "    def closest_prey_features(self, location):\n",
    "        nearest_prey = min(self.prey_locations, key=lambda reward: np.linalg.norm(location - np.array(reward)))\n",
    "        delta = tuple(np.array(location) - np.array(nearest_prey[0]))\n",
    "        distance = float(sum(delta) / 22)\n",
    "        angle = float(np.arctan2(delta[1], delta[0]))\n",
    "        return distance, angle\n",
    "        \n",
    "    def act(self, action):\n",
    "        next_location = tuple(np.array(self.location) + np.array(self.action_dict[action]['delta']))\n",
    "        \n",
    "        if self.grid[next_location] == 1:\n",
    "            reward = (100.0 + self.cost_per_step) if next_location in self.prey_locations else self.cost_per_step\n",
    "        else:\n",
    "            next_location = self.location\n",
    "            reward = float(self.cost_per_collision)\n",
    "\n",
    "        return next_location, reward\n",
    "    \n",
    "    def policy(self, env, prey_locations):\n",
    "        self.env = env\n",
    "        self.prey_locations = prey_locations\n",
    "        decaying_alpha = self.alpha\n",
    "        decaying_epsilon = self.epsilon\n",
    "        \n",
    "        action = int(self.epsilon_greedy_policy(self.location, self.epsilon))        \n",
    "        next_location, reward = self.act(action)\n",
    "        next_action = int(self.epsilon_greedy_policy(next_location, decaying_epsilon))\n",
    "        \n",
    "        self.learn(self.location, next_location, action, next_action, reward, decaying_alpha)\n",
    "        self.location = next_location\n",
    "        ## decaying_alpha = self.update_parameter(decaying_alpha, 0.9999, 0.05)\n",
    "        ## decaying_epsilon = self.update_parameter(decaying_epsilon, 0.9999, 0.05)\n",
    "\n",
    "    def training_policy(self, env, prey_locations):\n",
    "        self.env = env\n",
    "        self.prey_locations = prey_locations\n",
    "        decaying_alpha = self.alpha\n",
    "        decaying_epsilon = self.epsilon\n",
    "        \n",
    "        action = int(self.epsilon_greedy_policy(self.location, self.epsilon))        \n",
    "        next_location, reward = self.act(action)\n",
    "        next_action = int(self.epsilon_greedy_policy(next_location, decaying_epsilon))\n",
    "        \n",
    "        self.learn(self.location, next_location, action, next_action, reward, decaying_alpha)\n",
    "        self.location = next_location\n",
    "        return next_location, reward\n",
    "        # action = next_action\n",
    "        ## decaying_alpha = self.update_parameter(decaying_alpha, 0.9999, 0.05)\n",
    "        ## decaying_epsilon = self.update_parameter(decaying_epsilon, 0.9999, 0.05)\n",
    "\n",
    "    def learn(self, env, location, next_location, action, next_action, reward, alpha):\n",
    "        Q = float(self.Q_value(location)[action])\n",
    "        Q_next =  float(self.Q_value(next_location)[next_action])\n",
    "        TD_error = float(reward) + (float(self.gamma) * float(Q_next)) - float(Q)\n",
    "        self.theta[:, action] += float(alpha) * float(TD_error) * self.sense_features(env, location)            \n",
    "        return TD_error\n",
    "\n",
    "    def Q_value(self, location):\n",
    "        return np.dot(self.sense_features(location), self.theta)\n",
    "\n",
    "    def epsilon_greedy_policy(self, location, epsilon):\n",
    "        return int(np.random.randint(self.n_actions)) if np.random.rand() < epsilon else int(np.argmax(self.Q_value(location)))\n",
    "    \n",
    "    def update_parameter(self, parameter, decay_rate, parameter_min):\n",
    "        return max(parameter * decay_rate, parameter_min)\n",
    "    \n",
    "    def produce_training_plots(self, training, first_5_last_5, training_trials, trial_lengths):\n",
    "        if training:\n",
    "            self.plotter.plot_training_progress(trial_lengths=trial_lengths)\n",
    "        if first_5_last_5:\n",
    "            fig, axs = plt.subplots(2, 5, figsize=(10, 4))\n",
    "            for i in range(5):\n",
    "                axs[0, i] = self.plotter.plot_env(ax=axs[0, i])\n",
    "                axs[1, (4 - i)] = self.plotter.plot_env(ax=axs[1, (4 - i)])\n",
    "                axs[0, i] = self.plotter.plot_rewards(ax=axs[0, i])\n",
    "                axs[1, (4 - i)] = self.plotter.plot_rewards(ax=axs[1, (4 - i)])\n",
    "                \n",
    "                self.plotter.plot_episode(training_trials[i], ax=axs[0, i])\n",
    "                self.plotter.plot_episode(training_trials[len(training_trials) - i - 1], ax=axs[1, (4 - i)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multimodal_mazes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
