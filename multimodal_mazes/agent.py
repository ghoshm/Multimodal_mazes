# Agent

import numpy as np
import neat

# use numba: @numba.jit(nopython=True) - above def sense


class Agent:
    def __init__(self, location, channels, genome, config):
        """
        Creates a new agent.
        Arguments:
            location: initial position [r,c].
            channels: list of active (1) and inative (0) channels e.g. [0,1].
            genome: neat generated genome.
            config: the neat configuration holder.
        """
        self.location = np.array(location)
        self.channels = np.array(channels + [1])
        self.channel_inputs = np.zeros((5, len(self.channels)))
        self.action = []
        self.fitness = []
        self.sensors = [[0, 0, 0, -1, 1], [-1, 0, 1, 0, 0]]
        self.net = neat.nn.FeedForwardNetwork.create(genome, config)

    def sense(self, env):
        """
        Generates channel data for agent.
        Arguments:
            env: a np array of size x size x channels + 1.
            Where [:,:,-1] stores the maze structure.
        Updates:
            self.channel_inputs: a 5 (range) x (max channels + 1) np array.
            All agents will have max channels, but inactive ones will be zeroed.
        """
        # Generate channel data
        rc = [self.location[0] + self.sensors[0], self.location[1] + self.sensors[1]]

        # Generate channel data
        self.channel_inputs[:] = env[rc[0], rc[1], :]

        # Zero out inactive channels
        self.channel_inputs *= self.channels

    def act(self, env):
        """
        Updates the agent's state by one action.
        Arguments:
            env: a np array of size x size x channels + 1.
            Where [:,:,-1] stores the maze structure.
            channel_inputs: a 3 (range) x max channels np array, generated by sense.
        Updates:
            self.location: by one action.
            If the action collides with a wall it is ignored.
        """
        # Forward pass
        output = self.net.activate(list(self.channel_inputs.reshape(-1)))

        # Add noise to output (to avoid argmax bias)
        output += np.random.rand(len(output))

        # Choose action
        action = np.argmax(output)

        # Act
        if action == 0:  # left
            if env[self.location[0], self.location[1] - 1, -1] == 1.0:
                self.location += np.array([0, -1])
        elif action == 1:  # right
            if env[self.location[0], self.location[1] + 1, -1] == 1.0:
                self.location += np.array([0, 1])
        elif action == 2:  # up
            if env[self.location[0] - 1, self.location[1], -1] == 1.0:
                self.location += np.array([-1, 0])
        elif action == 3:  # down
            if env[self.location[0] + 1, self.location[1], -1] == 1.0:
                self.location += np.array([1, 0])
